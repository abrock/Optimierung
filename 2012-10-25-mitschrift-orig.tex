\chapter*{Formulierung von Parameterschätzproblemen}

englisch: Parameter Estimation

Beispiel: Chemische Reaktion: Bimolekulare Katalyse

\[A+B \to C\]

Erster Reaktionspfad: ohne Katalysator
Reaktionsgeschwindigkeitskoeffizient: $k_1 = f_1 \exp(-\frac{E_1}{RT})$

Zweiter Reaktionspfad: mit Katalysator
Reaktionsgeschwindigkeitskoeffizient: $k_2 = c_{kat} \exp(-\lambda t) f_2 \exp(-\frac{E_2}{RT})$

Reaktionsgeschwindigkeit: $r= k_1 c_1 c_2 + k_2 c_1 c_2$

Daraus ergibt sich ein DGL-System mit Anfangsbedingungen:

\begin{align*}
\dot n_1 &= -V(k_1+k_2) \frac{n_1}V \frac{n_2}V \\
\dot n_2 &= -V(k_1+k_2) \frac{n_1}V \frac{n_2}V \\
\dot n_3 &= -V(k_1+k_2) \frac{n_1}V \frac{n_2}V \\
n_1(0) &= n_{1,0} \\
n_2(0) &= n_{2,0} \\
n_3(0) &= 0 \\
\end{align*}

% ###### Bild 1

Der Prozessverlauf wird bestimmt von Steuergrößen, die von dem Experminentator eingestellt werden: $T,V,n_{1,0},n_{2,0}$ 
Außerdem hängt der Verlauf auch von den Größen $f_1,E_1,f_2,E_2,\lambda$, diese Größen sind durch Naturgesetze, Stoffeigenschaften etc. bestimmte Größen, diese nennen wir "Parameter", sie sind nicht zeitabhängig.

Experiment: Wähle Steuerungen, führe Prozess durch, erhebe Messdaten und Messfehler. Hier: Apparatur zur Messung von C.

% ###### Bild 2

Parameterschätzung: Bestimme die Werte der unbekannten Modellparameter so, dass die Simulation die Messwerte möglichst gut beschreibt.

\section*{Problemformulierung}

\subsection*{Prozessmodell}

\[\dot y = f(t,y(t),\overline p) \quad y(t_0) = y_0(\overline p) \q (2.1)\]

\subsection*{Modell für Beobachtungen}

Messzeitpunkte $t_i$, $i=1\cdot M$, nicht notwendigerweise verschieden. Messwerte $\eta_i \in \R$ und zugehörige Modellantworten $h_i (t_i, y(t_i), \overline p) \in \R$, Messfehler $\eps_i \in \R$ mit Standardabweichung $\sigma_i \in \R_+$

$\overline p$ seien die wahren, aber unbekannten Werte der Parameter

Annahme: Die Messfehler seien unabhängige, additive und normalverteilte Zufallsgrößen mit bekannter Standardabweichung:

\begin{align*}
\eta_i &= h_i(t_i, y(t_i), \overline p) + \eps_i \quad (2.2) \\
\eps_i &~ N(0, \sigma_i^2) \quad (2.3)
\end{align*}

Erwartungswert $0$ bedeutet, dass es keinen systematischen Messfehler gibt.

Bemerkung: Da die Messzeitpunkte nicht notwendig verschieden sind, kann es an einem Zeitpunkt $t$ auch mehrere Messungen geben. Auch die Messfunktionen $h_i$ sind nicht notwendig verschieden.

Beispiele für $\sigma:$

\begin{itemize}
\item absolute Messfehler: $\sigma_i \equiv \sigma$
\item relative Messfehler: $\ismga_i =  \frac x{100} \l| h_i(t_i,y(t_i),\overline p) \r| \sim \frac x{100} \l|\eta_i\r|$
\end{itemize}

Wichtig: Zur Beschreibung von Messdaten benötigt man Messwert $\eta$ und Genauigkeit $\sigma$.

Wenn man die wahren Parameter $\overline p$ nicht kennt kann man trotzdem immernoch die Residuen $\eta_i-h_i(t_i,y(t_i), \overline p)$, $i=1,\cdots,\M$ oder die gewichteten Residuen $\sigma_i^{-1}(\eta_i-h_i(t_i,y(t_i), \overline p))$, $i=1,\cdots,\M$ $(2.4)$  betrachten.

Ziel der Parameterschätzung: passe Modell an die Daten an.

Möglicher Ansatz: Maximum Likelihood, d.\,h. suche die Parameter, unter denen die beobachteten Daten die höchste Wahrscheinlichkeit haben. Das ist der ML-Schätzer $\dach p$.

\[\dach p = \argmax_p L(p)\]

Mit der Likelihood-Funktion $L(p)$, die die bedingte Wahrscheinlichkeit der Daten in Abhängigkeit von den Parametern angibt. Oft betrachtet man die sogenannte log-Likelihood-Funktion $\log L$. Für unabhängige, normalverteilte Messfehler

\begin{align*}
\dach p &= \argmax_p \log L(p)\\
&= \argmax_p \log \prod_{i=1}^M \frac 1{\sqrt{2\pi \sigma_i^2}} \exp\l( -\frac 12 \l(\frac{\eta_i-h_i(t_i,y(t_i),p)}{\sigma_i} \r)^2 \r) \\
&= \argmax_p \sum_{i=1}^M \log \frac 1{\sqrt{2\pi \sigma_i^2}} \exp\l( -\frac 12 \l(\frac{\eta_i-h_i(t_i,y(t_i),p)}{\sigma_i} \r)^2 \r) \\
&= \argmax_p \sum_{i=1}^M -\log \sqrt{2\pi \sigma_i^2} + \log \exp\l( -\frac 12 \l(\frac{\eta_i-h_i(t_i,y(t_i),p)}{\sigma_i} \r)^2 \r) \\
&= \argmax_p \sum_{i=1}^M \l( -\frac 12 \l(\frac{\eta_i-h_i(t_i,y(t_i),p)}{\sigma_i} \r)^2 \r) - \sum_{i=1}^M \log \sqrt{2\pi \sigma_i^2} \\
&= \argmax_p - \frac 12 \sum_{i=1}^M \l(\frac{\eta_i-h_i(t_i,y(t_i),p)}{\sigma_i} \r)^2 \\
&= \argmin_p \frac 12 \sum_{i=1}^M \l(\frac{\eta_i-h_i(t_i,y(t_i),p)}{\sigma_i} \r)^2 \quad (2.5)\\
\end{align*}

$\dach p$ minimiert als die $\|\cdot\|_2$-Norm des Residuen-Vektors. Gleichzeitig müssen $y$ und $p$ das AWP $(2.1)$ erfüllen.

Parameterschätzproblem:

\[\min \frac 12 \sum_{i=1}^M \l(\frac{\eta_i - h_i(t_i,y(t_i),p)}{\sigma_i}\r)^2\]

So dass gilt: $\dot y = f(t,y(t),p)$, $y(t_0) = y_0(p)$ (2.6).

Wichtig: Es werden immer Gewichte verwendet.

\begin{itemize}
\item Wenn man keine hat, macht man die Annahme, dass alle $\sigma_1 \equiv 1$
\item Ein gemeinsamer (positiver) Faktor an den verwendeten Gewichten ändert das Problen nicht.
\item Das Problem (2.6) und seine Lösung hängen sehr von den Gewichten ab.
\item Jede Wahl der Gewichte macht eine spezielle Annahme über die Statistik der Messfehler: $N(0,\sigma_i^2)$-verteilt.
\end{itemize}

Schreibweise:

\begin{align*}
\eta &= \begin{pmatrix} \eta_1 \\ \hdots \\ \eta_M \\
\h = h(y,p) &= \begin{pmatrix} \h_1(t_1,y(t_1), p) \\ \hdots \\ \h_M(t_M, y(t_M),p) \\
\eps &= \begin{pmatrix} \eps_1 \\ \hdots \\ \eps_M \\
S &= \begin{pmatrix} \sigma_1^2 & & 0 \\ & \?dots & \\ 0 & & \sigma_M^2
\end{align*}

Dann gilt: $\eps \sim N(0,S) \RA S^{-1/2} \eps \sim N(0,I)$

Das Zielfunktional von (2.6) minimiert

\[ \frac 12 \| S^{-\frac 12} (\eta-h)\|_2^2 = \frac 12 (\eta -h)^T S^{-1} (\eta-h) \quad (2.7) \]

$S$ heißt Kovarianzmatrix der Messfehler

\[S = E (\eps \eps^T) \quad \text{"Erwartungswert von $\eps\eps^T$"}\]

Für unabhängige normalverteilte Messfehler ist $S$ diagonal und positiv definit. Allgemeiner für unabhängige (korrelierte) multinomialverteile Messfehler ist $S$ symmetrisch und positiv definit.

\[ S=\begin{pmatrix} \sigma_1^2 & s_{ij} \\ s_{ji} & \sigma_M^2 \end{pmatrix} \]

mit Varianzen $\sigma_i^2$ und Kovarianzen $s_{ij}$

\begin{align*}
r_{ij} &= \frac{s_{ij}}{\sigma_i\sigma_j} \quad \text{Korrelation $-1 \leq r_{ij} \leq 1$} \\
R &= \begin{pmatrix} \sigma_1^{-1} & & 0 \\ ?dots \\ 0 & & \sigma_M^{-1} \end{pmatrix} S \begin{pmatrix} selbe matrix nochmal = \begin{pmatrix} 1 & & r_{ij} \\ & ?dots & \\ r_{ji} & & 1 \end{pmatrix} \\
\end{align*}

R heißt Korrelationsmatrix. Für nicht-diagonales $S$ ist die ML-Schätzung gegeben durch

\[ \min (eta-h(y,p))^T S^{-1} (\eta-h(y,p)) \quad (2.8)\]

Transformation: Zerlege $S$ = $LDL^T$ und minimiere folgendes:

\begin{align*}
\frac 12 (\eta -h)^T S^{-1} (\eta-h) &= \frac 12 (\eta-h)L^{-T} D^{-\frac 12} D^{-\frac 12} L^{-1} (\eta-h) \\
&= \frac 12 \|D^{-\frac 12} L^{-1}(\eta -h)\|_2^2 \quad (2.9) \\
D^{-\frac 12} L^{-1} (\eta -h(y,\overline p)) \sim N(0,I)
\end{align*}

Andere Messverteilungen:

Dichte:

\[\exp(-(\frac{|\eps|}{\sigma})^q) \quad (2.10)\]

Schätzer:

\[ \min \|S^{-\frac 12} (\eta-h(y,p)) \|_q \quad (2.11)\]

Beispiele:

\begin{align*}
q &= 1 \\
& \min \sum_{i=1}^M \l| \frac{\eta_i-h_i(y,p)}{\sigma_i} \r| \quad (2.12)
\end{align*}

"Robuste Parameterschätzung"


\begin{align*}
q &= \infty \\
& \min \max_{i=1,\cdots,M} \l\{ \l| \frac{\eta_i-h_i(y,p)}{\sigma_i} \r| \r\} \quad (2.13)
\end{align*}


"Worst-Case-Parameterschätzung"


Nebenbedingung des Parameterschätzproblems:

\item $y,p$ müssne die ODE-Modellgleichungen erfüllen
\ $\dot y = f(t,y,p)$
Variante: $y,z,p$ müssen ein DAE erfüllen $\dot y = f(t,y,z,p)$, $0=g(t,y,z,p)$
\item Anfangsbedingung: Konsistenzbedingung $y(t_0) = y_0(p)$, im DAE-Fall zusätzlich $0=g(t_0,y_0,z(t_0),p)$
\item Randbedingungen $r(y(t_0),y(t_end),p) = 0$ oder auch $r(y(t_0),y(t_1),\cdots,y(t_{K-1}, y(t_end),p) = 0$

Beispiele und Spezialfälle:

Periodizität: $y(t_0) = y(t_end)$

Innere-Punkt-Bedingung: $r(y(t_i),p) = 0$.

Linear gekoppelte Randbedingungen: $r(y(t_0),p) + r(y(t_1),p) + \cdots + r(y(t_end),p) = 0$.

In einem effizienten numerischen Code sollten solche speziellen Strukturen berücksichtigt werden.

% Benutzername: numerik2 Passwort: iwr432


