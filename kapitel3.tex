\chaptr{3. Shooting-Verfahren und Kollokation}

\msubsection{3.1 Single-Shooting: Einfachschießverfahren}

% Bild: Messdaten mit Fehlerbalken, Lösung des AWP für einen Parametersatz

Vorgehensweise:
\begin{itemize}
\item Wähle Werte für die Parameter $p$ ("`Initial Guess"')
\item Wir lösen das AWP $\dot y = f(t,y,p)$, $y(t_0) = y_0(p)$ $(3.1)$ mit einem numerischen Verfahren und erhalten eine Darstellung der Lösung $y(t; t_0 y_0, p)$
\item Setze die Lösung an den Messzeitpunkten in die Modellantwortsfunktionen ein und berechne $F_{1,i}(p) = \sigma_i^{-1}(\eta_i - h_i(t_i, y(t_i; t_0, y_0, p), p)$, $i=1,\cdots,M$ (3.2). Setze die Lösung außerdem an den Randbedingungspunkten in die Randbedingung ein: $F_2(p) = r(y(t_0; t_0, y_0, p), \cdots, y(t_k; t_0, y_o, p), p)$ (3.3). Halte dazu den Integrator an den Punkten $t_i$ an oder benutze die fehlerkontrollierte kontinuierliche Ausgabe.
\item Das ergibt ein endlichdimensionales nichtlineares Ausgleichsproblem, nämlich $\min \tfrac 12 \|F_1(p)\|_2^2$ so dass $F_2(p) = 0$ (3.4).
\item Löse diese mit einer geeigneten Methode. Kriterien: (3.5)
\begin{itemize}
\item iterativ, da das Problem nichtlinear ist
\item sollte unzulässige Iterierte erlauben, d.\,h. Zwischenwerte, bei denen $F_2(p) \neq 0$
\item sollte für Least-Squares-Zielfunktion geeignet sein
\end{itemize}
\end{itemize}

Wir benutzen daher das verallgemeinerte Gauß-Newton-Verfahren. Dieses benötigt folgende Ableitungen:

\begin{align*}
J_1(p) \colon&= \frac \partial{\partial p} F_1(p) \\
J_2(p) \colon&= \frac \partial{\partial p} F_2(p)
\end{align*}

In jeder Iteration des Gauß-Newton-Verfahrens muss also das AWP gelöst und $F_1$ und $F_2$ ausgewertet werden.

\emph{Bemerkung 3.1} berechnung von $J_1$ und $J_2$.

\begin{align*}
J_1(p) \colon&= \frac{\partial F_1}{\partial p} (p) \\
&= \frac{\partial}{\partial p} \l( \frac{\eta_i - h_i(\cdots)}{\sigma_i} \r)_{i=1,\cdots,M} \\
&= \l( -\frac 1 \sigma_i \l( \frac{\partial h_i}{\partial_y} (\cdots) \frac{\partial y}{\partial p}(t_i; t_0, y_0, p) + \frac{\partial h_i}{\partial p} (t_i; t_0, y_0, p) \r) \r)_{i=1,\cdots,M} \quad (3.6)\\
J_2(p) &= \frac{\partial F_2}{\partial p} (p) \\
&= \sum_{i=0^k} \frac{\partial r}{\partial y_i} \frac{\partial y}{\partial p} (t_i; t_0, y_0, p) + \frac{\partial r}{\partial p} \quad (3.7)
\end{align*}

Dazu berechnet man $\tfrac{\partial y}{\partial p} =\colon G_p$ als Lösung der VDE

\[ \dot G_p = \frac{\partial f}{\partial y} G_p + \frac{\partial f}{\partial p} \quad (3.8)\]

sowie die Ableitungen

\[ \frac{\partial f}{\partial y}, \frac{partial f}{\partial p}, \frac{\partial h_i}{\partial y}, \frac{\partial h_i}{\partial p}, \frac{\partial r}{\partial y_i}, \frac{\partial r}{\partial p}\]

der Modellfunktion per Hand, durch numerische Differentiation, oder durch automatische Differentiation.

\msection{Algorithmus 3.2: Verallgemeinertes Gauß-Newton-Verfahren}

Zur Lösung von $\min_p \tfrac 12 \|F_1(p)\|_2^2$ s.\,t. $F_2(p) = 0$

\begin{itemize}
\item Start mit einer Startschätzung $p^0$, $k=0$ ("`Initial Guess"')
\item Solange ein Abbruchkriterium verletzt ist:
\begin{itemize}
\item Berechne $\delta p^k$ durch Lösung des linearisierten Ausgleichsproblems $\min \tfrac 12 \|F_1(p^k) + J_1(p^k) \delta p\|_2^2$ s.\,t. $F_2(p^k) + J_2(p^k) \delta p = 0$ (3.9)
\item Bestimme eine Schrittweite $\alpha^k$, z.\,B. durch Linesearch 
\item Iteriere $p^{k+1} = p^k + \alpha^k \delta p^k$ (3.10)
\end{itemize} 
\end{itemize}

% Bei Newton wird die ganze Zielfunktion linearisiert, bei Gauß-Newton wird innerhalb der Norm linearisiert

Mögliches Abbruchkriterium: $\|\delta p^k \| \leq \eps$. Mehr zu Gauß-Newton-Verfahren in Kapitel\,\,4.

\msection{Algorithmus 3.3: Single-Shooting Gauß-Newton}

\begin{itemize}
\item Start mit einer Startschätzung $p^0$, $k=0$
\item Solange ein Abbruchkriterium verletzt ist:
\begin{itemize}
\item Integriere das AWP (3.1) zusammen mit der VDE (3.8) für $p=p^k$
\item Halte an den Punkten $t_i$ an und werte $F_1(p^k)$ und $F_2(p^k)$ gemäß (3.2) und (3.3) aus. Berechne $J_1(p^k)$ und $J_2(p^k)$ gemäß (3.6) und (3.7)
\item Berechne $\delta p^k$ durch Lösen des linearen Ausgleichsproblems (3.9).
\item Berechne eine Schrittweite $\delta p^k$ und iteriere gemäß (3.10)
\end{itemize}
\end{itemize}

Implementierung: Praktische Aufgabe 1

Benötigte Bestandteile für die Implementierung:

\begin{itemize}
\item Integrator für AWP/VDE: entweder durch Integrator mit Interner Numerischer Integration ("`IND"'), siehe Kapitel 5. Oder durch Integrieren des Systems
\[\begin{pmatrix} \dot y \\ \dot G_p \end{pmatrix} = \begin{pmatrix} f \\ \frac{\partial f}{\partial y} G_p + \frac{\partial f}{\partial p}\end{pmatrix}, \quad y(t_0) = \begin{pmatrix} y_0(p) \\ \frac{\partial y_0}{\partial p}(p) \end{pmatrix} \]
\item Löser für lineare Ausgleichsprobleme $\min \tfrac 12 \|F_1 + J_1 \delta x\|_2^2$ s.\,t. $F_2 + J_2 \delta x = 0$. KKT-Bedingung:
\[ \exists \lambda\colon \begin{pmatrix} J_1^T J_1 & J_2^T \\ J_2 & 0 \end{pmatrix} \begin{pmatrix} \delta x \\ \lambda \end{pmatrix} = -\begin{pmatrix} J_1^T F_1 \\ F_2 \end{pmatrix} \]
\item Globalisierungs-Strategie: BT-Linesearch oder $\alpha^j = 1$
\end{itemize}









\msubsection{Bemerkung 3.4}

Wen wir keine Randbedingung $r(\cdots)=0$ haben und die Differentialgleichung eine ODE und keine DAE ist dann lösen wir in jeder Iteration des Gauß-Newton-Verfahrens ("`GN-Verfahren"') für die aktuellen Paramter das komplette Simulationsproblem. Das Ausgleichsproblem ist dann unbeschränkt: $\min_p \tfrac 12 \|F_1(p)\|_2^2$. Das linearisierte Ausgleichsproblem $\min_{\Delta p} \tfrac 12 \|F_1 + \partial_1 \Delta p\|_2^2$ hat die Lösung $\Delta p = -(J_1^T J_1)^{-1} J_1^T F_1$. Berechnung über QR-Zerlegung von $J_1$.

\emph{Schwierigkeiten beim Single-Shooting:}

\begin{itemize}
\item In Satz 1.2 (Trompetenabschätzung) haben wir gesehen, dass kleine Störungen der Parameter sehr große Änderungen der Lösung $y$ des AWP und damit auch große Änderungen des Zielfunktions-Wertes und Nebenbedingungen des Parameter-Schätz-Problems zur Folge haben können. Wir sind dann "`weit weg"' von der Lösung und das GN-Verfahren konvergiert nicht, siehe auch Kapitel 4.
\item Wenn die Ableitungen $\tfrac{\partial f}{\partial y}$ nicht auf ganz $[t_0,\tend]\times R^{n_y} \times \R^{n_p}$  beschränkt bleiben kann es sein, dass für Parameter-Werte weit weg von den wahren Parametern die Lösung des AWP nicht auf dem ganzen Intervall $[t_0,\tend]$ existiert. Dann  ist das PS-Problem nicht auswertbar.
\item Die Vorinformation über die Trajektorien, die durch die Messwerte gegeben ist wird nicht genutzt.
\end{itemize}

\msection{3.2 Multiple Shooting, die Mehrzielmethode}

Zerlege das Integrationsintervall in Teilintervalle $t_0 = \tau_0 < \tau_1 < \cdots < \tau_m = \tend$. Die $\tau_i$ nennt man Mehrzielknoten. Löse das AWP nur auf diesen Intervallen $i=0,\cdots,m-1$: $\dot y = f(t,y,p)$, $t \in [\tau_i, \tau_{i+1} ]$ (3.10) mit den Anfangsbedingungen $y(\tau_i) = s_i$ (3.11). Erhalte die Lösung $y(t; \tau_i, s_i,p)$, $t\in[\tau_i, \tau_{i+1}]$ (3.12). Setze diese $y$ in die Messfunktion und Randbedingungs-Funktion an den entsprechenden Zeitpunkten $t_i$ ein. Die $s_i$, $i=0,\cdots,m-1$ sind zusätzliche Variablen des Problems. An den Mehrzielknoten $\tau_i$ formulieren wir zusätzliche  Stetigkeitsbedingungen, sog. "`Anschlussbedingungen"':

\[ s_{i+1} + y(\tau_{i+1}; \tau_i, s_i, p) = 0 \quad i=0,\cdots,m-2 \quad (3.13)\]

Für DAEs ist $s_i = (s_i^y, s_i^z)$ und im Punkt $\tau_i$ müssen die Konsistenzbedingungen $g(\tau_i, s_i^y, s_i^z, p) = 0$, $i=0,\cdots,m-1$ erfüllt sein (3.14). Anschlussbedingungen braucht man dann nur für die $s_i^y$ Das ergibt insgesamt das beschränkte, nichtlineare Ausgleichsproblem

\[ \min_x \frac  12 \|F_1(x)\|_2^2 \quad (3.15) \]

so dass $F_2(x) = 0$

\begin{itemize}
\item mit den Variablen $x = (s_0,\cdots,s_{m-1}, p)$ (3.16)
\item den Zielfunktionstermen
\[ F_{1,j} (x) = \frac 1{\sigma_j} \l( \eta_j - h_j(t_j,y(t_j; \tau_{i_j}, s_{i_j}, p), p) \r)  \quad (3.17) \]
mit $t_j \in [\tau_{i_j}, \tau_{i_j}]$, $j=1,\cdots,M$
\item den Nebenbedingungen $F_2(x)$, die bestehen aus:
\begin{itemize}
\item den Randbedingungen: $r(y(t_0;\tau_0,s_0,p),\cdots,y(t_k;\tau_{i_k},s_{i_k},p), p) = 0 $ (3.18) mit $t_j \in [\tau_{i_j},\tau_{i_j + 1}]$, $i=1,\cdots,k$
\item den Anschlussbedingungen $s_{i+1}+y(\tau_{i+1};\tau_i,s_i,p)=0$, $i=0,\cdots,m-2$ (3.19)
\item und für DAEs den Konsistenzbedingungen  $g(\tau_i, s_i^y, s_i^z,p)=0$, $i=0,\cdots,m-1$ (3.20).
\end{itemize}
\end{itemize}

Die Lösungen der Probleme (3.4) (Single-Shooting) und (3.15) (Multiple-Shooting) sind identisch. Das Problem (3.15) hat aber mehr Variablen.

Nachteile von Multiple-Shooting:

\begin{itemize}
\item Wesentlich höherer Programmieraufwand.
\item Mehraufwand bei der Bestimmung von Startwerten für die Variablen
\item Höherdimensionales Optimierungsproblem
\end{itemize}

Vorteile von Multiple-Shooting:

\begin{itemize}
\item Die Existenz einer (unstetigen) "`Start-Trajektorie"' auf dem gesamten Intervall $[t_0,\tend]$ ist gesichert.
\item Oft können die Multiple-Shooting-Variablen $s_i$ durch Messwerte initialisiert werden.
\item Man kann viel näher an der Lösung ($s_0^*,s_1^*,\cdots,s_{m-1}^*, p$) des Problems starten. Der Einfluss schlechter Startwerte für die $p$ wird abgemildert. Die Chance auf Konvergenz des GN-Verfahrens ist höher.
\item Die Nichtlinearität des Problems wird reduziert: Bei Single-Shooting $y(\tend; t_0,y_0,p)$ (3.21), bei Multiple-Shooting $y(t_{i+1};\tau_i,s_i,p)$ (3.22). Nach der Trompetenabschätzung ist die Nichtlinearität in (3.21) i.\,A. größer als in (3.22).
\end{itemize}

Zur Lösung von (3.15) verwenden wir wieder das verallgemeinerte GN-Verfahren und benötigen dazu die Ableitungen $J_1 = \partial_x F_1$ und $J_2 = \partial_x F_2$ (3.23). Diese haben die folgende Gestalt:

\[ \begin{pmatrix} J_1 \\ J_2 \end{pmatrix} = ........ \quad (3.22a)\]

mit

\begin{align*}
D_1^i &= \frac{\partial F_1}{\partial s_i} \quad i=0,\cdots,m-1 \\
D_1^p &= \frac{\partial F_1}{\partial p} & \text{Ableitung der ZF-Terme} \\
D_2^o &= \frac{\partial r}{\partial s_i} \quad i=0,\cdots,m-1 \\
D_2^p &= \frac{\partial r}{\partial p} & \text{ Ableitung der Randbedingungen}
\end{align*}

Ableitung der Anschlussbedingungen:

\begin{align*}
-s_{i+1} + y(\tau_{i+1}; \tau_i, s_i, p) &= 0 \quad i=0,\cdots,m-2 \\
\text{nach }s_i\colon \frac{\partial}{\partial s_i} y(\tau_{i+1}; \tau_i, s_i, p) &=\colon G_i \\
\text{nach }s_{i+1}\colon &-I \\
\text{nach }p\colon  \frac{\partial}{\partial p}y(\tau_{i+1}; \tau_i, s_i, p) &=\colon G_i^p & \text{ (sonst 0)}\\
\end{align*}

Ableitung der Konsistenzbedingungen:

\begin{align*}
H_i \colon&= \frac{\partial\{\partial s_i} g(\tau;i, s_i^y, s_i^z, p) \quad i=0,\cdots,m-1 \\
H_i^p &= \frac{\partial}{\partial p} g(\tau_i, s_i^y, s_i^z, p) \quad i=0,\cdots,m-1
\end{align*}

Lösungsverfahren müssen diese spezielle Struktur ausnutzen ("`Kondensierung"', siehe Kapitel 4). Die D-Matrizen können mit der Kettenregel berechnet werden, z.\,B.

\begin{align*}
\frac{\partial F_1}{\partial p} &= \sum_{j=1}^M \frac{\partial F_1}{\partial y(t_j)} \frac{\partial y(t_j)}{\partial p} + \frac{\partial F_1}{\partial p} \\
\frac{\partial F_1}{\partial s_i} &= \sum_{j=1}^M \frac{\partial F_1}{\partial y(t_j)} \frac{\partial y(t_j)}{\partial s_i}
\end{align*}

\msubsection{Bemerkung 3.5}

Der Aufwand zur Integration und zur Berechnung der $G_i$ ist bei Single-Shooting und Multiple-Shooting im Wesentlichen gleich.

\msection{3.3 Kollokation}

Literatur: Ascher, Mettheij, Russel: Numerical Solution of Boundary Value Problems for Ordinary Differential Equations

Biegler: Nonlinear Programming

\msubsection{Kollokations-Diskretisierung}

Approximiere die Lösung der ODE $\dot y = f(t,y,p)$ (3.24) durch stückweise Polynome vom Grad $k$ auf einem Gitter $t_0 = \tau_0 < \tau_1 < \cdots < \tau_{m-1} < \tau_m = \tend$ (3.25). Auf jedem dieser Teilintervalle wird die Lösung dargestellt durch

\begin{align*}
y_j^\tau (t; s_j) \colon&= \sum\limits_{l=0}^k s_j l \chi_l \l( \frac{t-\tau_j}{\tau_{j+1}-\tau_j} \r) & (3.26)\\
t &\in [\tau_j, \tau_{j+1}], \quad j=0,\cdots,m-1 \\
\text{Wobei } & \{ \chi_l \}_{l=k,\cdots,k} \text{ eine Basis des Polynomraums } P_k([0,1]) \text{ der Polynome vom Grad } \leq k \text{ ist}
\end{align*}

Die Koeffizienten $s_{jl} \in \R^{n_y}$, $j=0,\cdots,m-1$, $l=0,\cdots,k$, $s_j = (s_{j0},\cdots,s_{jk})$, $j=0,\cdots,m-$ (3.27) sind Variablen und bestimmt durch diese bedingungen:

\begin{itemize}
\item Die approximierte Lösung $y_j^\tau$ soll auf einer Unterteilung des Gitters die ODE erfüllen:
\[ t_{jl} \colon= \overline c_j + \rho_l h_j \quad l= 1,\cdots,k, \quad j=0,\cdots,m-1 (3.28) \]
(Kollokationspunkte) $\rho_l \in [0,1]$, $h_j \colon= \tau_{j+1}-\tau_j$. Also:
\[ \dot y_j^\tau (t_{jl}; s_j) = f(t_{jl}, y^\tau (t_{jl},s_j),p), \quad j=0,\cdots,m-1 \quad (3.29) \]
\item Die approximierte Lösung $y^\tau$ soll an den Gitterpunkten $\tau_j$ stetig sein:
\[ y_j^\tau (\tau_{j+1};s_j) = y_{j+1}^\tau(\tau_{j+1},s_{j+1}) \quad j=0,\cdots,m-2 \quad (3.30) \]
\item Anfangsbedingung:
\[y^\tau(\tau_0;s_0) = y_0(p) \quad (3.31) \]
\end{itemize}

Das sind $mk$ Kollokationsbedingungen, $m-1$ Stetigkeitsbedingungen und eine Anfangsbedingung, also $m(k+1)$ Bedingungen für die $m(k+1)$ Variablen $s_{jl}$.

\msubsection{Wahl der Polynombasis und der Kollokationspunkte}

z.\,B. B-Splines, Hermite-Splines. Wir benutzen die Runge-Kutta-Basis:

\begin{align*}
y_j^\tau(t;s_j) &= s_{jl} + h_j \sum\limits_{l=1}^k s_{jl} \Psi_l \l( \frac{t-\tau_j}{\tau_{j+1}-\tau_j} \r) & (3.32) \\
\text{mit } \Psi_l(0) &= 0 \\
\text{und } \dot\Psi_l(\rho_i) &= \begin{cases} 1 & i=l \\ 0 & \text{ sonst } \end{cases} \\
\Psi_l & \in \mathcal{P}_k([0,1])
\end{align*}

Dann lauten die Kollokationsbedingungen:

\begin{align*}
\dot y_j^\tau(t_{jl}; s_j) &= \dot y_j^\tau(\tau_j+\rho_l h_j; s_j) \\
&= f(t_{jl} y^\tau(t_{jl}; s_j),p) \quad l=1,\cdots,k & (3.33) \\
\end{align*}

und die Stetigkeitsbedingunng:

\begin{align*}
y_{j-1}^\tau (\tau_j; s_{j-1} - s_{j0} &= 0 & (3.34) 
\end{align*}

Die Basis $\{1,\Psi_1,\cdots,\Psi_k\}$ heißt Runge-Kutta-Basis, weil die $s_{jl}$, $l=1,\cdots,k$ dabei die Stufen des Runge-Kutta-Schemas

\begin{align*}
& \begin{pmatrix} \rho_1 | & \Psi_1(\rho_1) & \cdots & \Psi_k(\rho_1) \\ \vdots | & \vdots & & \vdots \\ \rho_k | & \Psi_1(\rho_k) & \cdots & \Psi_k(\rho_k) \\ & \Psi_1(1) & \cdots & \Psi_k(1) \end{pmatrix}  & (3.35)\\ 
\text{mit } \Psi_l(\rho) &= \int\limits_0^\rho L_l ( \overline \rho) \dd \overline \rho \\
\Psi_l(1) &= \int\limits_0^1 L_l (\rho) \dd \rho \\
\text{und } L_l(\rho) &= \prod\limits_{i \neq l} \frac{\rho-\rho_i}{\rho_l-\rho_i}
\end{align*}

sind

Man kann also in jedem Intervall $[\tau_j,\tau_{j+1})$ $y_j^\tau(\tau_{j+1}, s_j)$ als Ergebnis eines Schrittes eines impliziten Runge-Kutta-Verfahrens, gestartet bei $y_j^\tau (\tau_j, s_j)$ auffassen: "`Kollokationsschema"'

\msubsection{Konsistenzfehler:}

\begin{align*}
y(\tau_{j+1}) &= y(\tau_j) + h_j \sum\limits_{l=1}^k f(t_j + \rho_l h_j, y(\tau_j + \rho_l h_j), p) \int\limits_0^1 L_l(\rho) \dd \rho + h_j \mathcal O(h_j^q) & (3.36)
\end{align*}

Die Konsistenzordnung $q$ hängt ab von der Wahl der $\rho_l$, $l=1,\cdots,k$. Für Gauß-Punkte (Nullstellen der Legendre-Polynome) erhält man die maximal mögliche Ordnung $q=2(k-1)+2 = 2k$. Für die Lobatto-Punkte erhält man die Ordnung $q=2k-2$, "`Radau-Kollokations-Schema"'.

\msubsection{Beispiel Impliziter Euler, k=1}

\begin{align*}
y_j^\tau (t;s_j) &= s_{j0} + h_j s_{j1} \\
\end{align*}


Kollokationsbedingung:

\begin{align*}
s_{j1} &= f(\tau_{j+1} + h_j, s_{j0} + h_j s_{j1}, p) \\
\end{align*}

Stetigkeitsbedingung:

\begin{align*}
s_{j-1\,0} + h_{j-1}s_{j-1} - s_{j0} &= 0 
\end{align*}

\msubsection{Kollokation für beschränkte Parameterschätzprobleme}

Approximiere die Lösung der ODE durch eine Kollokationsdiskretisierung $y_j^\tau (t; s_j)$, $j=0,\cdots,m-1$ Setze diese an den Messzeitpunkten in die Least-Squares-Terme und an den Ranbedingungs-Punkten in die Randbedingung ein. Variablen sind:

\begin{align*}
x &= (s_0,s_1,\cdots,s_{m-1},p) \in \underbrace{ \R^{(k+1)n_y} \times \cdots \times \R^{(k+1)n_y}}_{m} \times \R^{n_p} \\
\end{align*}

$F_1$ besteht auf den Least-Squares-Termen, $F_2$ besteht au den Randbedingungen, den Kollokationsbedingungen (3.29), den Stetigkeitsbedingungen (3.30) und den Anfangsbedingungen (3.31). Es ergibt sich wieder ein beschränktes nichtlineares Ausgleichsproblem:

\[ \min_x \frac 12 \| F_1(x) \|_2^2 \quad s.\,t. \text{ } F_2(x) = 0 \quad (3.37) \]

Zur Lösung mit dem verallgemeinerten Gauß-Newton-Verfahren benötigen wir wieder

\begin{align*}
J_1 &= \frac{\partial F_1}{\partial x} \\
J_2 &= \frac{\partial F_2}{\partial y}
\end{align*}

% Wir stecken das einfach in einen General Purpose Sparse Matrix Solver rein, der findet das Sparsity Pattern raus und löst das

\msubsubsection{Bemerkung 3.6: Eigenschaften von $\begin{pmatrix} \partial_1 \\ \partial_2 \end{pmatrix}$}

\begin{itemize}
\item Es müssen keine Variations-DGL gelöst werden. Die Ableitungen nach $s$ und $p$ ergeben sich allein aus den Ableitungen der Kollokationspolynome und Ableitungen von $f$, Messfunktionen und Randbedingungen.
\item Das System ist sehr groß, aber auch sehr dünn besetzt.
\end{itemize}

\msubsubsection{Vorteile von Kollokation}

\begin{itemize}
\item Es werden keine Integratoren und Variations-DGL-Löser benötigt.
\item Es können Standard-Sparse-Löser eingesetzt werden
\item Für lineare DGL, lineare Messfunktion und lineare Randbedingungen ist das Ausgleichs-Problem linear.
\item Simulationsproblem und Optimierungsproblem werden in einer einzigen Schleife gelöst: All-At-Once
\end{itemize}

\msubsubsection{Nachteile von Kollokation}

\begin{itemize}
\item Die DGL wird nicht adaptiv diskretisiert
\item Das Problem ist sehr hochdimensional
\end{itemize}

% Das ist jetzt mehr so ein Laberkapitel

\msection{3.4 Ansätze zur Optimierung von DGL-Modellen}

Wir haben bisher kennengelernt:

\begin{itemize}
\item Single-Shooting ("`Black-Box-Ansatz"')
\item Multiple Shooting 
\item Kollokation
\end{itemize}

Das Optimierungsproblem muss iterativ gelöst werden.

Die konzeptionellen Unterschiede sind:

\begin{itemize}
\item Beim Single-Shooting wird in jeder Optimierungsiteration die DGL des AWP komplett gelöst. Das ist der sogenannte sequentielle Ansatz.
\item Bei Multiple Shooting wird die Differentialgleichung in jeder Optimierungsiteration nur auf Teilintervallen gelöst, das AWP wird relaxiert, d.\,h. wir lassen während der Iterationen unstetige Lösungen zu. Erst im Lösungspunkt wird das AWP gelöst, simultan mit Nebenbedingungen und der Optimalität. Das ist ein Beispiel für einen simultanen Ansatz.
\item Bei Kollokation wird die DGL vollständig diskretisiert und gemeinsam mit dem Optimierungsproblem gelöst. Das nennt man den All-At-Once-Ansatz.
\end{itemize}

Unsere Optimierungsverfahren lassen unzulässige Iterierte zu. Die Lösungen dieser Formulierungen sind gleich, da die Formulierungen mathematisch äquivalent sind. Die Iterationen der Verfahren können sich aber stark unterscheiden. Es werden während der Optimierung Schritte in verschieden hochdimensionalen Suchräumen berechnet. Dies kann bessere Konvergenzeigenschaften bedeuten.

Verallgemeinerung: Lifting von beliebigen nichtlinearen Problemen in höherdimensionale Räume:

\begin{align*}
x^{16} &= 2 \\
&\LRA \\
x_1^2 - x_2 &= 0 \\
x_2^2 - x_3 &= 0 \\
x_3^2 - x_4 &= 0 \\
x_4^2 -  2  &= 0 \\
\end{align*}

siehe Albersmeyer, Diehl: The lifted Newton Method and its Apllication in Optimization

\msection{3.5 Relaxierte Formulierung von DAEs}

DAE:

\begin{align*}
\dot y &= f(t,y,z,p) \\
0 &= g(t,y,z,p) & \text{ Annahme: Index 1}\\
\text{Anfangsbedingung: } y(t_0) &= y_0, \quad z(t_0) = z_0\\
\text{Konsistenzbedingung: } 0 &= g(t_0, y_0, z_0, p) \\
\end{align*}

Für numerische Lösungsverfahren werden konsistente Anfangswerte benötigt. Dafür gibt es zwei Ansätze:

\begin{itemize}
\item Berechnung konsistenter Anfangswerte durch Lösen der Konsistenzbedingung mit z.\,B. einem Newton-Verfahren. Kann Schwierigkeiten bereiten, weil das Newton-Verfahren zunächst nur lokal konvergiert und globalisiert werden muss, z.\,B. mit Homotopie-Ansätzen. Das muss vor jeder Integration der DAE erfolgen.
\item Integration mit "`inkonsistenten"' Anfangswerten, relaxierte Formulierung:
\begin{align*}
\dot y &= f(t,y,z,p) & (3.40) \\
0 &= g(t,y,z,p) - \beta(t) g(t_0,y_0,z_0,p) \\
y(t_0) &= y_0 \\
z(t_0) &= z_0 \\
\text{mit } \beta(t_0) &= 1 \\
\text{z.\,B. } \beta(t) &\equiv 1 \\
\text{oder } \beta(t) &= e^{-\alpha(t-t_0)} \\
\end{align*}
\end{itemize}

Das modifizierte Problem (3.40) ist automatisch konsistent und kann numerisch integriert werden. Interpretation: Die Nebenbedingungen geben eine Mannigfaltigkeit vor, auf der die Lösung liegt. In der relaxierten Formulierung ist diese Mannigfaltigkeit verschoben. Die Konsistenzbedingung $g(t_0,y_0,z_0,p) = 0$ (3.41) löst man im übergeordneten Optimierungsproblem als Nebenbedingung mit.

Bei Single-Shooting wird die Konsistenzbedingung im Anfangszeitpunkt erfüllt, dafür wird das Gleichungssystem gelöst.

Bei Multiple-Shooting wird für jedes Mehrzielintervall eine relaxierte Konsistenzbedingung eingesetzt. In jedem Mehrzielknoten: $g(\tau_i, s_i^y, s_i^z, p)=0$ für $i=0,\cdots,m-1$.

Bei Kollokation wird kein Integrator verwendet. Die algebraischen Nebenbedingungen werden zusätzlich in den Kollokationspunkten gefordert. (in RK-Basis-Darstellung):

\begin{align*}
y_j^\tau (t_{jl}; s_j) &= s_{jl}^y \\
&= f(t_{jl}, y_j^\tau(t_{jl};s), s_{jl}^z,p) \\
0 &= g(t_{jl}, y_j^\tau(t_{jl}; s_j), s_{jl}^z, p) \\
\text{mit } s_{jl}^y & \cong \dot y(t_{jl}), \quad s_{jl}^z \cong z(t_{jl})
\end{align*}

Stetigkeitsbedingung nur für $s^y$:

\[ y_{j-1}^\tau (\tau_j; s_{j-1})-s_{j_0}^y = 0 \]

Die Bestimmung konsistenter Werte für die algebraischen Gleichungen wird von der Simulationsaufgabe in die Ebene des Optimierungsproblems verlagert.






