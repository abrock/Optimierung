% Wiederholen schadet nicht und vielleicht verstehen Sie es dieses Mal

\chaptr{5. Lokale Konvergenz von Newton-Typ-Verfahren}

Newton-Typ-Verfahren:

\bitm
\item Löse iterativ mit einem guten Startwert $x_0 \in \R^n$
\item Löse in jeder Iteration ein linearisiertes Problem
\item Wende eine Globalisierungsstrategie an
\eitm

\[ F\colon \R^n \to \R^m \quad M(x)\colon \text{ Lösungsoperator des linearen Problems} \]

\msection{Algorithmus 5.1 (Newton-Typ-Verfahren)}

\bitm
\item Startwert $x^0$, $k := 0$
\item Solange ein geeignetes Abbruchkriterium verletzt ist:
\bitm
	\item Berechne $\Delta x^k := - M(x^k) F(x^k)$ (5.1)
	\item Berechne $\alpha^k$ auf einer Globalisierungsstrategie
	\item Iteriere $x^{k+1} := x^k + \alpha^k \Delta x^k$ (5.2)
\eitm
\eitm

Kann angewendet werden:

\bitm
\item Zur Bestimmung von Nullstellen von $F(\cdot)$, $m=n$ $\RA$ Newton-Verfahren oder Quasi-Newton-Verfahren
\item Insbesondere zur Bestimmung von Nullstellen von $\Delta L(x,\lambda) = 0$ (notwendige Optimalitätsbedingung) $\RA$ SQP-Verfahren.
\item Zur Bestimmung von Lösungen unbeschränkter nichtlinearer Ausgleichsprobleme $\min \frac 12 \|F(x)\|_2^2$, $m \geq n$
\item Zur Bestimmung von Lösungen beschränkter nichtlinearer Ausgleichsprobleme $\min \frac 12 \|F_1(x)\| \text{ s.\,t. } F_2(x) = 0$, $m = m_1+m_2 \geq n$, $m_2 \leq n$ $\RA$ Verallgemeinertes Gauß-Newton-Verfahren.
\eitm

\[ \text{Sei } J := \frac{\partial F}{\partial x} \quad (5.3) \text{ Jacobimatrix} \]

\msubsection{Bemerkung 5.2}

\bitm
\item Bei Newton-Verfahren ist $M(x) = J(x)^{-1}$ die Inverse von $J$.
\item bei Quasi-Newton-Verfahren ist $M(x) \cong J(x)^{-1}$
\item Bei SQP-Verfahren ist $M(x, \lambda) = \nabla_{x,\lambda}^2 L(x, \lambda)^{-1}$ oder 
$M(x, \lambda) \cong \nabla_{x,\lambda}^2 L(x, \lambda)^{-1}$
\item Bei Gauß-Newton-Verfahren ist $M(x) = (J(x)^T J(x))^{-1} J(x)^T$ die Moore-Penrose-Pseudoinverse.
\item Bei Verallgemeinerten Gauß-Newton-Verfahren ist \[ M(x) = \bpm I & 0 \epm \bpm J_1(x)^T J_1(x) & J_2(x)^T \\ J_2(x) & 0 \epm^{-1} \bpm J_1(x)^T 0 \\ 0 & I \epm \] die verallgemeinerte Inverse von $J$.
\eitm

\msection{Satz 5.3: Lokaler Kontraktionssatz (Bock 1987)} % Enorm wichtig

\[ \text{Sei } F\colon D \subset \R^n \to \R^m, \quad F \in \C^1(D,\R^m), \quad J := \frac{\partial F}{\partial x} \]

Für alle $x,y \in D$ mit $y-x = -M(x) F(x)$ und $\theta \in [0,1]$ gelte:

\bitm
\item Es existiert ein $\omega < \infty$ so dass $\| M(y) (J(x+\theta(y-x)) - J(x)) (y-x) \| \leq \omega \theta \|x-y\|^2$ (5.4)
\item Es existiert ein $\kappa(x) \leq \kappa < 1$, so dass $\| M(y) R(x) \| \leq \kappa(x) \|y-x\|$ (5.5) für das Residuum $R(x) := F(x) - J(x)M(x)F(x)$
\item Sei $x_0 \in D$ gegeben mit $\Delta x^j := -M(x^j) F(x^j)$, $\delta_j := \kappa + \frac \omega 2 \| \Delta x^j \|$, $\delta_0 = \kappa + \frac \omega 2 \|\Delta x^0 \| < 1$ (5.6)
\item \[ D^0 := \l\{ z : \|z-z_0\| \leq \frac{\|\Delta x^0\|}{1-\delta 0} \r\} \subset D \]
\eitm

Dann gilt:

\bitm
\item a) Die Iterierten $x^{j+1} = y^j + \Delta x^j$ sind wohldefiniert und bleiben in $D^0$.
\item b) Es existiert ein $x^* \in D^0$ so dass $x^j \to x^*$ ($j \to \infty$)
\item c) \[ \| x^{j+k} - x^*\| \leq \frac{\Delta x^0}{1-s_j} s_j^k \text{ "`hoch k"' (a-priori-Abschätzung)} \]
\item d) \[ \| \Delta x^{j+1} \| \leq s_j \| \Delta x^j \| = \kappa \| \Delta x^j \| + \frac \omega 2 \| \Delta x^j\|^2 \text{ (5.7)} \]
\eitm

Beweis:

\begin{align*}
\| \Delta x^{j+1} \| &= \| M(x^{j+1}) F(x^{j+1}) \| =: \| M^{j+1} F^{j+1} \| \\
&= \| M^{j+1} (F^{j+1} - F^j - J^j \Delta x^j) + M^{j+1} R^j \| \\
& \leq \| M^{j+1} \l( \int\limits_0^1 J(x^j + t \Delta x^j) \Delta x^j \dd t - \int\limits_0^1 J^j \Delta x^0 \dd t \r)\| + \|M^{j+1} R^j \| \\
&\leq \int\limits_0^1 \| M^{j+1} (J(x^j + t \Delta x^j) - J^j) \Delta x^j \| \dd t + \|M^{j+1} R^j \| + \kappa \| \Delta x^j \| \\
&\leq \int\limits_0^1 \omega t \| \Delta x^j \|^2 \dd t + \kappa \|\Delta x^j \| \\
&= \frac \omega 2 \|\Delta x^j\|^2 + \kappa \| \Delta x^j \| \\
&= \delta_j \| \Delta x^j \| \RA d) \\
\end{align*}

Zeige: $(\delta_j)_{j \in \N}$, $(\|\Delta x^j\|)_{j \in \N}$ sind monoton fallend:

Induktion: 

\begin{align*}
\delta_j - \delta_{j+1} &= \frac \omega 2(\|\Delta x^j\| - \|\Delta x^{j+1}\|) \\
&\geq \frac \omega 2 (\|\Delta x^j\|-\delta_j \|\Delta x^j\|) \\
&\geq \frac \omega 2 \|\Delta x^j\| (1-\delta_j) > 0\\
\| \Delta x^{k+k} \| &\leq \delta_{j+k} \| \Delta x^{j+k-1} \| \\
& \leq \delta_{j+k-1} \cdots \delta_j \|\Delta x_j\| \\
& \leq \delta_j^k \|\Delta x^j\| \\
\| x^{j+2} - x^0 \| & \leq \|\Delta x^j+1\| + \cdots + \|\Delta x^0\| \\
&\leq ( \delta_0^{j+1} + \cdots + \delta s_0^1) \|\Delta x^0\| \\
&\leq \frac{1}{1-\delta_0} \|\Delta x^0\| \RA a)
\end{align*}


$(x^j)_{j \in \N}$ ist Cauchy-Folge:

\begin{align*}
\| x^{i+j+1} - x^i \| & \leq \sum_{k=0}^j \| \Delta x^{i+k} \\
&\leq \sum\limits_{k=0}^j \delta_j^k \| \Delta x^i\| \\
&\leq \sum\limits_{k=0}^i \delta_0^{i+k} \|\Delta x^0\| \\
&\leq \delta_0^i \l(\sum\limits_{k=0}^\infty \delta_0^k \r) \| \Delta x^0 \| \\
&= \delta_0^i \frac{\|\Delta x^0\|}{1-\delta_0} \to 0 (i \to \infty \\
& \RA x^j \to x^* \\
D \text{ kompakt } & \RA x^* \in D^0 \RA b)
\end{align*}

Beweis:

\begin{align*}
\|x^j+k+i-x^{j+k}\| & \leq \frac{\|\Delta x^{j+k}\|}{1-\delta_{j+k}} \\
&\leq \delta_j^k \frac{\|\Delta x^j\|}{1-\delta_j}
\end{align*}

$\forall i \geq 0$ also auch für $i \to \infty$:

\begin{align*}
\|x^* - x^{j+k} \| & \leq \delta_j^k \frac{\|\Delta x^j\|}{1-\delta_j} \RA c) 
\end{align*}

\msubsection{Korollar 5.4}

Für $F(x)=0$, $F\colon \R^n \to \R^m$ konvergiert das Newton-Verfahren mit $M(x) = J(x)^{-1}$ lokal quadratisch.

Beweis:

\begin{align*}
\| M^{j+1} R^j \| &= \| (J^{j+1})^{-1} (I - J^j (J^j)^{-1}) F^j\| = 0 \RA \kappa = 0 \\
\|\Delta x^{j+1} \| & \leq \frac \omega 2 \| \Delta x^j \|^2 \\
\|x^{j+1} - x^* \| &= \|x^j - x^* + \Delta x^j \| = \| M^j ( J^j(x^j-x^*) - (F^j - F^*)) \| \quad \text{ (*): Im Lösungspunkt}\\
&= \l\| M^j \int\limits_0^1 (J^j-J(x^j + t(x^j-x^*))) (x^j-x^*) \dd t \r\| \quad \text{(HDI)} \\
&\leq \underbrace{\|M^j J^*\|}_{\leq \Gamma} \int\limits_0^1 \| M^*(J^j-J(x^j+t(x^j-x^*)))(x^j-x^*) \| \dd t \\
&\leq \Gamma \int\limits_0^1 \omega t \|x^j - x^*\|^2 \dd t \\
&= \Gamma \frac \omega 2 \|x^j - x^*\|^2 \\
\end{align*}

% Tafel geht kaputt,... "Das ist mir jetzt eigentlich egal"

\msubsection{Bemerkung 5.5: Quasi-Newton-Verfahren}

Für näherungsweise Newton-Verfahren ("`Quasi-Newton-Verfahren"') ist $x^{j+1} = x^j - M(x^j) F(x^j)$ mit $M(x^j) \cong J(x^j)^{-1}$ $\kappa > 0$, zur Konvergenz muss $\kappa < 1$ sein:

\begin{align*}
\|M^{j+1} R^j\| &= \|M^{j+1} ((M^j)^{-1} - J^j) M^j F^j \| \\
&= \| M^{j+1} ((M^j)^{-1} - J^j) (x^{j+1} - x^j) \| \\
&\leq \kappa \|<x^{j+1} x^j\| \\
\end{align*}

Notwendig für Konvergenz ist also

\begin{align*}
\|M^j\| & \leq \gamma \text{ und } \|(M^j)^{-1}-J^j\| \text{ klein} 
\end{align*}

\msubsection{Satz 5.6 (Dennis-Mor\'e}

Sei $F\colon \R^n \to \R^n$ stetig differenzierbar. Betrachte die Iteration $x^{j+1} = x^j + \Delta x^j$ und sei $\Delta x^j$ gegeben durch $\Delta x^j = -M(x^j) F(x^j)$. Wir nehmen an, dass die Folge der $x^j$ gegen einen Punkt $x^j$ mit $F(x^j) = 0$ konvergiert mit $J(x^*)$ regulär. Dann konvergiert $(x^j)_{j\in \N}$ Q-superlinear gegen $x^*$, d.\,h. $\lim_{j\to \infty} \frac{\|x^{j+1} - x^*\|}{\|x^j-x^*\|} = 0$ genau dann wenn

\[ \lim_{j \to \infty} \frac{\| (M(x^j)^{-1} - J(x^*)) \Delta x^j \|}{\|\Delta x^j\|} = 0 \quad (5.8) \]

Beweis: siehe Vorlesung Algorithmische Optimierung 1.

\msubsection{Varianten von Quasi-Newton-Verfahren}

\[ \Delta x^j = -M(x^j) F(x^j), \quad M(x^j) \cong J(x^j)^{-1} \]

\bitm
\item Berechne $J(x^j)$ durch Differenzenquotienten
\item Halte $M(x^j)$ fest
\bitm
	\item für alle Iterationen: $M(x^j) = J(x_0)^{-1}$
	\item für einige Iterationen: $M(x^j) = J(x^{\ov j})^{-1}$ solange $\frac{\Delta x^{j+1}}{\|\Delta x^j\|} \leq \delta$ z.\,B. $\delta = \frac 14$, danach neues $\overline j = j$ 
\eitm
\item Nähere $J(x^j)$ bzw. $M(x^j)$ durch Update-Formeln aus $J(x^{j-1})$ bzw. $M(x^{j-1})$ an, siehe unten.
\eitm

\msubsection{Bemerkung 5.7 (Bedeutung von $\omega$)}

\[ \| M(y) (J(x+t(y-x)) - J(x)) (x-y) \| \leq t \omega \|y-x\|^2 \]

\bitm
\item Wenn $M(y)$ in einer Umgebung von $x^*$ beschränkt ist: $\|M(y)\| \leq \gamma$, $\gamma < \infty$ und
\item $J$ eine Lippschitz-Bedingung erfüllt: $\|(J(x+t(x-y))-J(x)) (y-x)\| \leq \beta t \|y-x\|^2$, $\beta < \infty$, dann ist $\omega = \gamma \beta < \infty$ in einer Umgebung von $x^*$
\eitm

$\omega$ kann sehr groß werden wenn

\bitm
\item $\|M\|$ sehr groß ist, d.\,h. $J$ ist fast singulär.
\item die erste Ableitung von $J$ bzw. die zweite Ableitung von $F$ groß ist, d.\,h. das Problem ist sehr nichtlinear.
\eitm

\msubsection{Anwendung auf (verallgemeinerte) Gauß-Newton-Verfahren:}

\[ M(x) = J^+(x) = \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} \bpm J_1^T & 0 \\ 0 & I \epm \]

Es gelte (CQ) und (PD). $\|M\|$ ist groß, wenn

\[ \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm \]

fast singulär ist.

Residuum:

\begin{align*}
R(x) &= F(x) - J(x) M(x) F(x) \\
&= F(x) + J(x)\Delta x
\end{align*}

Im Lösungspunkt: $\Delta x^* = 0$: $R(x^*) = F(x^*)$,

\begin{align*}
\| R(x^*) \| &= \| F_1(x^*) \|, \quad \text{ da } F_2(x^*) = 0
\end{align*}

\msubsection{Bemerkung 5.8: Bedeutung von $\kappa$} 

Wenn $M(x)$ stetig differenzierbar ist gilt in $D^0$:

\begin{align*}
\|M(y) - M(x)\| & \leq L \|y-x\| \text{ und } \\
\|R(x)\| &= \|F(x) - J(x)M(x) F(x) \| \leq \rho \\
\end{align*}

Außerdem ist $M(x) R(x) = (\underbrace{M(x) - M(x)J(x) M(x)}_{= 0 \text{ (4. Moore-Penrose-Axiom)}} F(x) = 0$

\begin{align*}
\| M(y) R(x) \| &= \| (M(y)-M(x)) R(x) \| \leq \rho L \|y-x\| \\
\kappa &= \rho L 
\end{align*}
























































