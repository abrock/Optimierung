\chaptr{11 Optimale Versuchsplanung}

\msection{11.1 Anwendungsbeispiel Urethan-Reaktion}

\begin{align*}
A + B & \stackrel{1}{\to} C \\
A + C &\stackrel{2,3}{\LRA} D \\
3 A & \stackrel{4}{\to} E
\end{align*}

\bitm
\item A: Isocyanat
\item B: Butanol
\item C: Urethan
\item D: Allophanat
\item E: Isozyanurat
\item L: Dimethylsulfonid
\eitm

TODO: Bild und so

DAE-Modell: $n_1, \cdots, n_6$: Molzahlen von $A,\cdots,L$

\begin{align*}
\dot n_3 &= V (r_1 - r_2 + r_3) \\
n_3(t_0) &= 0 \\
\dot n_4 &= V(r_2 - r_3) \\
n_4(t_0) &= 0 \\
\dot n_5 &= V r_4 \\
n_5(t_0) &= 0 \\
0 &= n_1 + n_3 + 2n_4 + 3n_5 - n_{1,0} - n_{1,0,Z_1} feed_1(t) \quad \text{ $Z_1$: Zulauf 1 } \\
0 &= n_2 + n_3 + n_4 - n_{2,0} - n_{2,0,Z_2} feed_2(t) \\
0 &= n_6 - n_{6,0} - n_{6,0,Z_1} feed_1(t) - n_{6,0,z_2} feed_2(t) 
\end{align*}

Algebraische Gleichungen ergeben sich aus Stöchioetrie.

\begin{align*}
\dot T &= \dd T (t) \\
T(t_0) &= T_0 \\
\dot{feed}_1 &= dfeed_1(t) \\
feed_1(t_0) &= 0 \\
\dot{feed}_2 &= dfeed_2(t) \\
feed_2(t_0) &= 0 \\
\end{align*}

Reaktionsgeschwindigkeiten:

\begin{align*}
r_1 &= k_1 \frac{n_1}V \frac{n_2}V \\
r_2 &= k_2 \frac{n_1}V \frac{n_3}V \\
r_3 &= k_3 \frac{n_4}V \\
r_4 &= k_4 \l( \frac{n_1}V \r)^2
\end{align*}

Geschwindigkeitskoeffizienten:

\begin{align*}
k_i &= k_{ref} \exp \l( -\frac{E_{ai}}R \l( \frac 1{T(t)} - \frac 1{T_{refi}} \r) \r) \quad i = 1,2,4 \\
\frac{k_2}{k_3} &= K_{c,2} \exp \l( - \frac{\Delta H_2}{R} \l( \frac{1}{T(t)} - \frac1{T_{g2}} \r) \r) \\
\end{align*}

Volumen:

\[ V = \sum\limits_{i=1}^6 \frac{M_i n_i}{\rho_i} \]

Das Modell enthält:

\bitm
\item 9 Zustände: $n_1, \cdots, n_6, T, feed_1, feed_2$
\item 8 unbekannte Parameter: $k_{ref1}, E_{a1}, k_{ref2}, E_{a2}, k_{ref4}, E_{a4}, K_{c2}, \Delta H_2$ 
\item 3 Steuerfunktionen $u(t)$: $\dd T(t), dfeed_1(t), dfeed_2(t)$
\item 8 Steuergrößen: $n_{1,0}, n_{2,0}, n_{6,0}, n_{1,0,Z_1}, n_{6,0, Z_1}, n_{2,0,Z_2},n_{6,0,Z_2}$
\item Restliche Größen sind Konstanten: $R, T_{ref}, M_i, \rho_i$
\eitm

Ziele:

\bitm
\item Schätzungen der Modellparameter auf experimentellen Daten.
\item Design der Experimente, so dass die geschätzten Parameter möglichst kleine statistische Unsicherheiten haben, also optimale Versuchsplanung.
\eitm

\msection{Statistische Analyse der Lösung des Parameterschätzproblems}

Modellgleichung (\obda ODE-RWP).

\begin{align*}
\dot y &= f(t,y(t),p,q,u(t)) \quad (11.1) \\
y(t_0) &= y_0(p,q) 
\end{align*}

\bitm
\item $p \in \R^{n_p}$: Parameter, werden durch die Natur bestimmt und sind erstmal unbekannt bzw. müssne geschätzt werden.
\item $q \in \R^{n_q}$: Steuergrößen und $u\colon [\tn, \tend] \to \R^{n_u}:$ Steuerfunktionen werden durch den Experimentator festgelegt.
\item $r(t_1, y(t_1), \cdots, t_k, y(t_k), p, q) = 0$: Randbedingungen (11.2)
\eitm

\msubsection{Modell für die Messungen}

\begin{align*}
\eta_i &= h_i(t_i, y(t_i), p, q) + \eps_i, \quad i = 1,\cdots,M \quad (11.3) \\
\eps_i & \sim \mathcal N \l( 0, \frac{\sigma_i^2}{w_i} \r), \cov (\eps_1, ¸eps_j) = 0, i\neq j \\
\end{align*}

$w_i \in \{0,1\}:$ Wird Messung $i$ tatsächlich durchgeführt? Für gegebene Daten: alle $w_i=1$. Für zu planende Experimente: Die Wahl von $w$ entscheidet über die Durchführung der Messung $i$.

\bitm
\item $w_1 = 1$: $\eps_i \sim \mathcal N(0, \sigma_i^2)$
\item $w_i = 0$: Die Messung $i$ hat unendliche Varianz.
\item $w_i \in [0,1]$, \zb $w_i = \frac 12$. "`Halbe Messung"' mit doppelter Varianz ($\to$ sinnvolle Relaxierung).
\eitm

Parameterschätzproblem:

\begin{align*}
\min_{p,y} \frac 12 &\sum\limits_{i=1}^M \frac{w_i}{\sigma_i^2} \l( \eta_i - h_i(t_i, y(t_i), p, q) \r)^2 \\
\text{s.\,t. } \dot y &= f(t,y,p,q,u) \\
y(t_0) &= y_0(p,q) \\
r(t_1, y(t_1), \cdots, t_k, y(t_k)) &= 0 \\
\text{mit } \xi :&=  (q,u,w) \text{ fest} \\
\end{align*}

Nach Parametrisierung der Dynamik, diehe Kapitel 3:

\begin{align*}
v &= (p,s) \\
\min_{v} \frac 12 \|F_1(v,\xi)\|_2^2 \quad (11.5) \\
\text{s.\,t. } 0 &= F_2(v, \xi) \\
F_1(v,\xi) &= \l( \frac{\sqrt{w_i}}{\sigma_i} \l( j_i(t_i,y(t_i, v, \xi), p, q) \r) \r)_{i=1,\cdots,M} \\
\end{align*}

$F_2(v,\xi)$ enthält die Randbedingungen und die zusätzlichen Gleichungsbeschränkungen aus der Parametriierung.

Jacobi-Matrix:

\begin{align*}
\partial_{1ij} &= \frac{\partial F_{1i}}{\partial v_j} = - \frac{\sqrt{w_i}}{\sigma_i} \l( \frac{\partial h_i}{\partial y} \frac{\partial y(t_i)}{\partial v_j} + \frac{\partial h_i}{\partial v_j} \r) \\
\partial_{2ij} &= \frac{\partial F_{2i}}{\partial v_j} = \frac{\partial F_{2i}}{\partial y} \frac{\partial y}{\partial v_j} + \frac{\partial F_{2i}}{\partial v_j} \\
\end{align*}

$\frac{\partial y}{\partial v}$ sind Lösungen von Variationsdifferentialgleichungen nach Parametern und Anfangswerten.

Verallgemeinerte Inverse:

Lineares Problem:

\begin{align*}
\min &\frac 12 \|J_1 \Delta v + F_1\|_2^2 \\
\text{s.\,t. } 0 &= J_2 \Delta v + ?????
\end{align*}

Lösungsoperator:

\begin{align*}
J^\dagger &= \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} \bpm J_1^T & 0 \\ 0 & I \epm \\
&\text{unbeschränkter Spezialfall: } J^\dagger &= (J_1^T J_1)^{-1} J_1^T \\
\Delta v &= -J^\dagger F, \quad F = \bpm F_1 \\ F_2 \epm
\end{align*}

Im Beispiel: Messwerte gegeben für Massenprozente von $A,C,D,E$ mit verschiedenen Varianzen an 5 Messzeitpunkten.

TODO: Bildchen

Ergebnis: Gesamtresiduum wird reduziert von $3\cdot 10^6$ auf $4\cdot 10^1$, die Kurven werden gut gefittet $\to$ "`Guter Fit"'

Aber: Ist das auch eine gute Schätzung der Parameter?

Es gilt: die Messdaten sind Zufallsgrößen. Der Lösungsalgorithmus des Parameterschätzproblems bildet die Messdaten auf den Parameterschätzer $\hat v_{ab}$. Also sind auch die geschätzten Parameter Zufallsgrößen.

TODO: mehr Bildchen wagen

Wir betrachten die Fehlerfortpflanzung für das linearisierte Parameterschätzproblem im Lösungspunkt $\hat v$. Dann gilt:

\[ \hat v \sim \mathcal N(v,C) \]

(ist die Kovarianzmatrix des Parameterschätzers).

Erwartungswert $E$:

\begin{align*}
E(\hat v) &= E(v_{wahr} + \Delta v) = E(\vwahr) + E(-J^\dagger F) \\
&= E(\vwahr) - J^\dagger E(F) = E(\vwahr) - J^\dagger E \bpm S^{-\frac 12} (\eta-h) \\ F_2 \epm \\
&= E(\vwahr) - J^\dagger 0 = \vwahr \\
\text{mit } S &= \diag \l( \frac{\sigma_i^2}{w_i}, i=1,\cdots,M \r) \\
\eta &= \bpm \eta_1 \\ \hdots \\ \eta_M \epm \\
h &= \bpm h_1 \\ \hdots \\ h_M \epm \\
C &= E(\hat v \hat v^T) = E( (\vwahr + \Delta v) (\vwahr + \Delta v)^T ) = E(\Delta v \Delta v^T) \\
&= E(J^\dagger F F^T (J^\dagger)^T \\
&= J^\dagger E \bpm F_1 F_1^T & F_1 F_2^T \\ F_2 F_1^T & F_2 F_2^T \epm (J^\dagger)^T \\
&= J^\dagger \bpm E(F_1 F_1^T) & 0 \\ 0 & 0 \epm (J^\dagger)^T \\
\text{mit } E(F_1 F_1^T) &= E( S^{-\frac 12} (\eta-h)(\eta-h)^T S^{-\frac 12}) \\
&= S^{-\frac 12} E(\eps \eps^T) S^{-\frac 12 } \\
&= S^{-\frac 12} S S^{-\frac 12} = I \\
\end{align*}

Also: $\hat v$ ist multinomialverteilt mit Erwartungswert $\vwahr$ und Kovarianzmatrix $C = J^\dagger \bpm I & 0 \\ 0 & 0 \epm (J^\dagger)^T$.

\msubsection{Bemerkung 11.1: Eigenschaften von C}

\bitm
\item a) $C$ existiert und ist eindeutig, wen CQ und PD erfüllt sind.
\item b) $C$ ist positiv semidefinit
\item c) $C$ hängt von Ableitungen des Parameterschätzproblems ab.
\item d) Wenn das Modell nichtlinear in $v$ ist hängt $C$ von den Parametern $\hat v$ ab.
\item e) $C$ hängt von dern Versuchsplanungsgrößen $\xi = (q,u,v)$ ab.
\item f) $C$ hängt nicht von den Messwerten ab.
\eitm

Unbeschränkter Fall:

\[ J^\dagger = (J_1^T J_1)^{-1} J_1^T \]

\[ C = (J_1^T J_1)^{-1}J_1^T J_1 (J_1^T J_1)^{-1} = (J_1^T J_1)^{-1} \]

Beschränkter Fall:

\[ J^\dagger = \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm \bpm J_1^T & 0 \\ 0 & I \epm \]

\[ C = \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} \bpm J_1^T J_1 & 0 \\ 0 & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm \bpm I \\ 0 \epm \quad (11.7) \]

\msubsection{Lemma 11.2}

Sei

\[ \bpm X & Y^T \\ Y & Z \epm := \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} =:M^{-1} \]

Dann ist die Kovarianzmatrix $C=X$ und erfüllt

\begin{align*}
I &= J_1^T J_1 C + J_2^T Y \\
0 &= J_2 C
\end{align*}

Beweis: Multipliziere Matrix mit $M^{-1}$:

\begin{align*}
I &= J_1^T J_1 X + J_2^T Y \\
0 &= J_1^T J_1 Y + J_2^T Z \\
0 &= J_2 X \\
I &= J_2 Y^T \\
\end{align*}

Setze in (11.7) ein:

\begin{align*}
C &= \bpm I & 0 \epm M^{-1} \bpm J_1^T J_1 & 0 \\ 0 & 0 \epm M^{-1} \bpm I \\ 0 \epm \\
&= \bpm X & Y^T \epm \bpm J_1^T J_1 & 0 \\ 0 & 0 \epm \bpm X \\ Y \epm \\
&= X J_1^T J_1 X\\
&= X (\underbrace{J - J_2^T Y}_{=J_1^T J_1 X}) \\
&= X - (\underbrace{J_2 X}_{=0})^T Y \\
&= X \\
\end{align*}

\msubsection{Korollar 11.3}

\begin{align*}
C &= \bpm I & 0 \epm \bpm J_1^T J_1 J_2^T \\ J_2 & 0 \epm^{-1} \bpm I \\ 0 \epm \\
\end{align*}

\msection{Konfidenzgebiete}

Das $(100\cdot\alpha)\%$-Konvergenzgebiet ist das Gebiet, in dem die wahren aber unbekannten Parameter $\vwahr$ mit einer Sicherheit von $\alpha \in (0,1]$ liegen. Als Näherung betrachten wir das Konfidenzgebiet im Lösungspunkt $\hat v$ des Gauß-Newton-Verfahrens:

\begin{align*}
G(\alpha, \hat v) &= \{ v \in \R^n : F_2(x) = 0 \wedge \|F_1(v)\|_2^2 - \|F_1(\hat v)\|_2^2 \leq \gamma(\alpha)^2 \} \\
\text{mit } \gamma(\alpha)^2 &= \Chi_{n-n_2}^2 (a-\alpha)
\end{align*}

(Quantil der $\Chi^2$-Verteilung zum Wert $\alpha$ mit $n-n_2$ Freiheitsgraden, $v \in \R^n$, $F_1(v) \in \R^{n_1}$, $F_2(v) \in \R^{n_2}$).

\msubsection{Linearisierte Näherung}

\begin{align*}
G_L(\alpha) &= \{ v \in \R^n : F_2(\hat v) + J_2(\hat v) (v-\hat v) = 0, \\
& \|F_1(\hat v) + J_1(\hat v)(v-\hat v) \|_2^2 - \|F_1(\hat v)\|_2^2 \leq \gamma(\alpha)^2 \} \quad (11.10) \\
\end{align*}

\msubsection{Lemma 11.4}

Das linearisierte Konfidenzgebiet lässt sich in folgender Form schreiben:

\[ G_L(\alpha, \hat v) = \ov G_L(\alpha, \hat v) := \l\{ v \in \R^n, v - \hat v = J^\dagger(\hat v) \bpm \delta w \\ 0 \epm, \delta w \in \R^{n_1}, \|\delta w\|\leq \gamma(\alpha) \r\} \]

Beweis: siehe \zb Skript zu Numerik 3.

\msubsection{Korollar 11.5}

Sei $\theta_i = \gamma(\alpha) \sqrt{c_{ii} ( \hat v) }$, $i=1,\cdot,n$. Dann gilt: $G_L(\alpha, \hat v)$ wird durch einen Quader mit den Seitenlängen $2\theta_i$ eingeschlossen:

\[ G_L(\alpha,\hat v) \subseteq [ \hat v_1 - \theta_1, \hat v_1 + \theta_1] \times \cdots \times [\hat v_n - \theta_n, \hat v_n + \theta_n ] \quad (11.12)\]

Beweis:

\begin{align*}
|v_1-\hat v_1| &\leq \l\| \l( J^\dagger(\hat v) \bpm I \\ 0 \epm \r)^T e_i \r\| \|\delta w\|_2 \\
&= \l( e_i^TJ^\dagger(\hat v) \bpm I \\ 0 \epm \bpm I & 0 \epm J^\dagger (\hat v)^T e_i \r)^{\frac 12} \|\delta w\|_2 \\
&\leq \sqrt{c_{ii}(\hat v)} \gamma(\alpha) \\
\end{align*}

\msection{11.3 Das Versuchsplanungs- und Optimierungsproblem}

Wie kann die Güte des parameterschätzproblems verbessert werden?

Unbeschränkter Fall: $C = (J_1^T J_1)^{-1}$ Konfidenzintervall $\sqrt{c_{ii}}$. Mehr Messungen durchführen:

\begin{align*}
C_+ &= \l( \bpm J_1 \\ \hdots \\ J_1 \epm^T \bpm J_1 \\ \hdots \\ J_1 \epm \r)^{-1} = (m J_1^T J_1)^{-1} = \frac 1m (J_1^T J_1)^{-1} \\
\end{align*}

Die Konfidenzintervall werden nur um den Faktor $m^{-\frac 12}$ kleiner: $\sqrt{c_{+ii}} = \sqrt{m^{-1} c_{ii}} = m^{-\frac 12} \sqrt{c_{ii}}$.

Stattdessen: Entweder Experimente, die die statistische Unsicherheit der Parameterschätzung minimieren.

Versuchsplanungsgrößen: $\xi = (q,u,w)$:

\bitm
\item Steuergrößen $q \in \R^{n_q}$
\item Steuerfunktionen $u\colon [\tn, \tend] \to \R^{n_u}$
\item Auswahl der Messungen $w\in \{0,1\}^M$
\eitm

\[ w_i = \begin{cases} 1 & \text{ Messung wird durchgeführt} \\ 0 & \text{ Messung wird nicht durchgeführt} \end{cases} \]

Zielfunktion: Informationsfunktion auf der Kovarianzmatrix $C$ ($C$ ist positiv semidefinit, symmetrisch).

Üblich sind:

\bitm
\item $\Phi(C) = \sqrt[n]{\det C}$ D-Kriterium 
\item $\Phi(C) = \frac 1n \mathrm{Spur} C$ A-Kriterium
\item $\Phi(C) = \|C\|_2 = \lambda_{max} (C)$ E-Kriterium
\item $\Phi(C) = \max_{i=1,\cdots,n} \sqrt{c_{ii}}$ U-Kriterium
\eitm

\msubsection{11.6}

Statt auf $C$ kann man die Kriterien auch auf $K^T C K$, die Projektion auf den $n_k$-dimensionalen Teilraum, $K \in \R^{n\times n_k}$ vollrangig, anwenden. Insbesondere für das D-Kriterium, wenn $C$ nicht positiv definit ist.

\msubsection{Bemerkung 11.7}

Im unbeschränkten Fall korrespondiert das D-Kriterium zum Volumen des Konfidenzellipsoids, das A-Kriterium zur Länge der durchschnittlichen Halbachsen, das E-Kriterium zur längsten Halbachsenlänge und das M-Kriterium zum größten Konfidenzintervall.

\msubsection{Optimale Versuchsplanung}

Entwirf Experimente, die die statistische Unsicherheit der Parameterschätzung minimieren. 

Versuchsplanungsgrößen: $\xi = (q,u,w)$

\msubsection{Versuchsraum}

Nebenbedingungen

\bitm
\item Steuerbeschränkungen: $q_i \in [L_{q_i}, U_{q_i}]$, $u_i(t) \in [L_{u_i}, U_{u_i}]$, $L_c \leq c(q) \leq U_c$
\item Zustandsbeschränkungen: $L_b \leq b(t,y(t),p,q,u(t)) \leq U_b$ $\forall t \in [\tn, \tend]$. Spezialfall: $L_{y_i} \leq y_i(t) \leq U_{y_i}$.
\item Auswahl von Messungen: Sei $I_j \subseteq \{1,\cdots,M\}$ eine Teilmenge aller möglichen Messungen, \zb alle Messungen an einem Zeitpunkt, alle Messungen mit einem bestimmten Verfahren. $L_{I_j} \leq \sum_{i\in I_j} w_i \leq U_{I_j}$.
\item Kostenbeschränkung (lineare Kosten): \[ \sum\limits_{i=1}^M c_i^w w_i + \sum\limits_{i=1}^{n_q} c_i^q + \sum\limits_{i=1}^{n_u} \int\limits_\tn^\tend c_i^u(t) u(t) \dd t \leq C_{max}\]
\bitm
\item $c_i^w$: Kosten pro Messung
\item $c_i^q$: Kosten pro Einheit Steuergröße
\item $c_i^u$: Kosten für Steuerfunktion
\eitm
\eitm

Insgesamt:

\bitm
\item Ungleichungsbeschränkungen: $L \leq \Psi(t,y(t),p,q,u(t),w) \leq U$ (11.16)
\item Gleichungsbeschränkungen: $\Chi (t,y(t),p,q,u(t),w) = 0$ (11.17)
\item Ganzzahligkeitsbedingungen: $w_i \in \{0,1\}$, $i=1,\cdots,M$.
\item Variationsdifferentialgleichung für $\frac{\partial y}{\partial v} =: G$
\eitm

Versuchsplanungsproblem (11.19)

\begin{align*}
\min_{q,u,w} &\Phi \l( \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} \bpm I \\ = \epm \r) \\
\text{s.\,t. } J_1 &= \l( - \frac{\sqrt{w_i}}{\sigma_i} \l( \frac{\partial h_i}{\partial y} G(t_i) + \frac{\partial h_i}{\partial v} \r) \r)_{i=1,\cdots,M} \\
J_2 &= \frac{\partial F_2}{\partial y} G + \frac{\partial F_2}{\partial v} \\
\dot y &= f(t,y,p,q,u), \quad y(t_0) &= y_0 \\
r(t_1, y(t_1), \cdots, t_k, y(t_k), p, q) &= 0 \\
\dot G &= \frac{\partial f}{\partial y} G + \frac{\partial f}{\partial v}, \quad G(t_0) &= \frac{\partial y_0}{\partial v} (p,q) \\
L & \leq \Psi(t,y(t),p,q,u(t),w) \leq U \\
0 &= \Chi(t,y(t),p,q,u(t),q) \\
w_i & \in \{0,1\} \quad i = 1,\cdots, M \\
\end{align*}

Eigenschaften:

\bitm
\item Optimalsteuerungsproblem
\item mit ODE und VDE-Nebenbedingung
\item Nicht-Standard-Zielfunktion: hochgradig nicht-separiert
\item Ganzzahligkeitsbedingungen
\eitm

\msubsection{Varianten}

\bitm
\item Mehrfachexperimente: Ein Experiment besteht aus einer Durchführung des Prozesses für gegebene Versuchsplanungsgrößen $\xi = (q,u,w)$. Oft benötigt man aber Daten aus mehreren Experimenten, um alle Parameter zu schätzen.
Wir erlauben, dass die Experimente durch verschiedene Modelle beschrieben werden, diese müssen nur die Parameter gemeinsam haben. $j=1,\cdots,N_{ex}$. Pro Experiment:
\bitm
\item Modellgleichungen: $\dot y^j = f(t,y^j,p,q^j,u^j)$, $y^j(t_0^j) = y_0^j(p,q^j)$, $r^j(t_1^j, y^j(t_1^j), \cdots, t_{k^j}^j, y^j(t_{k^j}^j), p, q^j) = 0$ mit $t \in [t_0^j, t_{end}^j]$
\item Messungen: $\eta_i^j = h_i^j (t_i^j, y^j(t_i^j),p,q_i^j) + \eps_i^j$, $i = 1,\cdots,M^j$, $\eps_i^j \sim \mathcal N \l(0, \frac{{\sigma_i^j}^2}{w_i^j} \r)$, $\cov (\eps_i^j, \eps_k^l) = 0$, $i\neq k$ oder $j\neq l$
\item
\eitm
\eitm

Führt auf Mehrfachexperiment-Parameterschätz-Problem:

\begin{align*}
\min_{p,s^1, \cdots, s^{n_{ex}}} &\sum\limits_{j=1}^{N-{ex}} \|F_1^j(p,s^j)\|_2^2 \quad (11.20) \\
\text{s.\,t. } F_2^j(p,s^j) &= 0 \quad j = 1,\cdots, N_{ex} \\
\end{align*}

Struktur. $F^h$ hängt nur von $s^j$ ab. Jacobi-Matrizen $J_i$, $i=1,2$:

\begin{align*}
J_i &= \frac{\partial F_i}{\partial (p,s)} = \bpm J_{ip}^1 & J_{is^1}^1 & 0 & \cdots & 0 \\ \vdots & & \ddots \\ \vdots &&& \ddots \\ J_{ip}^{N_{ex}} & 0 & 0 & \cdots & J_{is^{N_{ex}}}^{N_{ex}}  \epm
\end{align*}

\msubsection{Feste und variable Experimente}

$N_{ex} = N_{fix} + N_{var}$

\bitm
\item $N_{fix}$: Bereits durchgeführte Experimente, für die schon Messdaten vorliegen und deren Sensitivitäten in der Versuchsplanung als Vorinformationen berücksichtigt werden sollen.
\item $N_{var}$: neu zu planende Experimente, deren $\xi$-Variablen in der Versuchsplanung optimal bestimmt werden sollen.
\eitm

\begin{align*}
J_i &= \bpm J_{ip}^{fix} & J_{is^{fix}}^{fix} & 0 & \cdots & 0 \\ \vdots && \ddots \\ \vdots &&& \ddots \\ J_{ip}^{var} & 0 & 0 &\cdots & J_{is^{var}}^{var} \epm i=1,2 \\
J_1^T J_1 &= \bpm H_{pp} & H_{s^1p}^T & \cdots & H_{s^{N_{ex}}p}^T \\ H_{s^1p} & H_{s^1 s^1} & & 0 \\ \vdots && \ddots  & \vdots\\ H_{s^{N_{ex}}p} & 0 & \cdots & H_{s^{N_{ex}}s^{N_{ex}}} \epm \\
H_{pp} &= \sum\limits_{j=1}^{N_{ex}} {J_{1p}^j}^T J_{1p} \\
&= \underbrace{\sum\limits_{j=1}^{N_{fix}} {J_{1p}^j}^T J_{1p}}_{=: H_{pp}^{fix} \text{ fest}} + \underbrace{\sum\limits_{j=N_{fix+1}}^{N_{fix} + N_{var}} {J_{1p}^j}^T J_{1p}}_{\text{variabel}} \\
\end{align*}

\msubsection{Mehrfachexperiment-VP-Problem}

\bitm
\item Für die alten Experimente $j=1,\cdots,N_{fix}$ sind die $q^j, u^j, w^j$ Konstanten. Es müssen keine Steuer-, Zustands-, Ganzzahligkeits- oder Kostenconstraints berücksichtigt werden
\item Für die neuen Experimente $j = N_{fix} +1, \cdots, N_{fix} + N_{neu}$ sind die $q^j, u^j, w^j$ Variablen und die Nebenbedingungen müssen berücksichtigt werden.
\eitm

\begin{align*}
\min & \Phi \l( \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} \bpm I \\ 0 \epm \r) \\
\text{s.\,t. } J_1 &, J_2 \text{ wie oben} \\
\end{align*}

ODE-RWP für Experimente $q,\cdots,N_{ex}$. VDE für Experimente $1,\cdots,N_{ex}$, Ungleichungs- Gleichungs- und Ganzzahligkeitsbedingungen für Experimente $N_{fix}+1, \cdots N_{fix}+N_{var}$ (11.25) 

\msubsection{Bemerkung 11.8}

Das Mehrfachexperiment-VP-Problem (11.25) minimiert die statistische Unsicherheit einer PS aus allen (alten und neuen) Experimenten. Dabei werden die neuen Experimente so ausgelegt, dass sie gegenüber den alten einen maximalen Zuwachs an statistischer Güte ergeben.

Sequentielle Vorgehensweise:

Wenn die Modellantworten $h_i(t_i,y(t_i,p),p,q)$ nichtlinear in $p$ sind, d.\,h. $J_1 = J_1(p)$ hängt auch $C$, das VP-Problem und der optimale Versuchsplan von den aktuellen Werten von $p$ ab.

Wir führen sukzessive und abwechselnd VP und PS durch:

\bitm
\item Startparametersatz
\item Versuchsplanung
\item Vorschlag für neue Experimente
\item Versuchsdurchführung
\item Messdaten
\item Parameterschätzung
\item Verbesserter Parametersatz
\item Versuchsplanung
\item $\cdots$
\eitm

\msubsection{Bemerkung 11.11}

\bitm
\item in der Versuchsplanung werden Mehrfach-Experimente geplant unter Berücksichtigung der alten Experimente aus der vorherigen Iterationen und mit Optimierung ggf. neuer Experimente
\item Die Parameterschätzung wird mit allen Daten aus allen bisherigen Iterationen durchgeführt.
\item Die Versuchsplanung kann robustifiziert werden (s.\,u.). Der Robustifizierungsanteil wird automatisch zurückgefahren. 
\item Wenn nötig können in den Iterationen auch Modellmodifikationen vorgenommen werden.
\item Man stoppt, wenn die Parameter hinreichend gut bestimmt sind, oder wenn das experimentelle Budget erschöpft ist.
\eitm

\msection{Robustifizierung der Versuchsplanung gegenüber Parameterunsicherheit}

Wenn $h$ nichtlinear in $p$ ist, ist $J = J(p)$ eine Funktion der Parameter und damit auch $C=C(p)$. Für schlecht bekannte Parameter können also auch schlechte Experimente berechnet werden.
Unsicherheit der Parameter: (a-priori-Information) $p\sim \mathcal N(p_0, \Sigma)$, $\Sigma$ positiv definit mit Konfidenzgebiet $\{p:\|p-p_0\|_{2,\Sigma^{-1}} \leq \gamma \}$ mit der Norm
$\|\cdots\|_{2,\Sigma^{-1}}$ aus dem Skalarprodukt $\la x,y\ra = x^T \Sigma^{-1} y$. Worst-Case-Design:

\[ \min_{\xi \in \Omega} \max_{\|p-p_0\|_{2,\Sigma^{-1}} \leq \gamma} \Phi(C(\xi,p)) \]

Das ist ein semi-infinites Optimierungsproblem. Die Lösung erfordert die Berechnung von globalen Optima von endlichdimensionalen Teilproblemen, das ist NP-schwer.

Vereinfachte Problemstellung: Taylorentwicklung nach $p$ um $p_0$:

\[ \min_{\xi \in\Omega} \max_{\|p-p_0\|_{2,\Sigma^{-1}} \leq \gamma} \Phi(C(\xi,p_0)) + \frac{\partial}{\partial p} \Phi(C(\xi,p_0))(p-p_0) \quad (11.27) \]

Das innere Problem kann jetzt explizit gelöst werden:

\msubsection{Lemma 11.9}

\begin{align*}
\max_{\|p-p_0\|_{2,\Sigma^{-1}} \leq \gamma} &\Phi(C(\xi, p_0)) + \frac{\partial }{\partial p} \Phi(C(\xi,p_0)) (p-p_0) \\
&= \Phi(C(\xi,p_0)) + \gamma \l\| \frac{\partial}{\partial p} \Phi(C(\xi,p_0)) \r\|_{2,\Sigma} \quad (11.28) \\
\end{align*}

Beweis:

Sei $\Delta p := p-p_0$, $a := \frac{\partial}{\partial p} \Phi(C(\xi,p_0))^T$. Das Minimum wird am Rand angenommen, betrachte

\[ \min_{\|\Delta p\|_{2,\Sigma^{-1}}^2 = \gamma^2} a^T \Delta p \]

Mit der Lagrangefunktion 

\begin{align*}
L(\Delta p, \lambda) &= a^T \Delta p + \lambda(\Delta p^T \Sigma^{-1} \Delta p - \gamma^2) \\
\nabla_p L(\Delta p^*, \lambda) &= a + 2 \lambda \Sigma^{-1} \Delta p^* = 0 \RA \Delta p^* = \frac{-\Sigma a}{2\lambda} \\
\Delta_\lambda L(\Delta p^*, \lambda) &= \Delta {p^*}^T \Sigma^{-1} \Delta p \gamma^2 = 0 \\
&= \frac{a^T \Sigma a}{(2\lambda)^2} - \gamma^2 \\
\RA \lambda &= \pm \frac{\|A\|_{2,\Sigma}}{2\gamma}
\end{align*}

Positive Lösung: Minimum. Maximum:

\[ \Delta p^* = \gamma \frac{\Sigma a}{\|a\|_{2,\Sigma}} \]

Zielfunktionswert:

\[ a^T \Delta p^* = \gamma \frac{a^T \Sigma a}{\|a\|_{2,\Sigma}} = \gamma \|a\|_{2,\Sigma} \]

Robustifiziertes Versuchsplanungsproblem:

\[ \min_{\xi\in \Omega} \underbrace{\Phi(C(\xi,p_0))}_{\text{nicht robustes Gütekriterium}} + \gamma \underbrace{\l\| \frac{\partial}{\partial p} \Phi(C(\xi,p_0)) \r\|_{2,\Sigma}}_{\text{Sensitivität des Gütekriteriums nach den Parametern}} \]

Gewichtung durch das Konfidenzlevel der Parameterunsicherheit.

\msubsection{Bemerkung 11.10}

\bitm
\item Gegenüber der nichtrobusten Formulierung kommt eine zusätzliche Ableitung nach $p$ hinzu.
\item Parameterabhängige Nebenbedingungen müssen ebenfalls robustifiziert werden.
\eitm

\msubsection{11.4 Numerische Methoden zur Lösung von Versuchsplanungsproblemen}

\bitm
\item Direkter Ansatz der optimalen Steuerung
\item Diskretisierung der Zustandsbeschränkungen
\item Parametrisierung der Dynamik durch Single Shooting oder mit einer geeigneten Umformulierung durch Multiple Shooting:
\[ \min \Phi\l( \bpm I & 0 \epm \bpm H_m J_{2m}^T \\ J_{2m} & 0 \epm^{-1} \bpm I \\ 0 \epm \r) \\
\text{s.\,t. } H_j = H_{j-1} + \sum\limits_{i: t_j \in [\tau_i, \tau_{i+1})} J_{1j.}^T J_{j1.} \\
= H_{j-1} + \sum\limits_{i: t_j \in [\tau_i, \tau_{i+1})} \l( -\frac{\sqrt{w_i}}{\sigma_i} \frac{\partial h_i}{\partial y} G(t_i) + \frac{\partial h_i}{\partial v} \r)^T e_i^T e_i \l( - \frac{\sqrt{w_i}}{\sigma_i} \frac{\partial h_i}{\partial y} G(t_i) + \frac{\partial h_i}{\partial v} \r) \\
H_0 = 0 \\
J_{2j} = J_{2j-1} + \sum\limits_{i: t_i \in [\tau_i, \tau_{i+1})} \l( \frac{\partial F_2}{\partial y} G(t_i) + \frac{\partial F_{2i}}{\partial v} \r) e_i\\
i=1,\cdots,m \quad J_{20} = 0 \]
+ Multiple-Shooting-Parametrisierung von ODE und VDE. Dann ist die ZF vom Mayer-Typ, das Problem kann auf Teilintervallen separiert ausgewertet werden. Aber: viele zusätzliche Variablen $H_0$, $J_{2j}$. Strukturen müssen ausgenutzt werden $\RA$ Aktuelle Doktorarbeit Dennis Janka.
\item Behandlung der Ganzzahligkeit: \\
0-1-Variablen. $w_i \in \{0,1\}$, $i=1,\cdots,M$. Relaxierung. $w_i \in [0,1]$, $i=1,\cdots,M$ $\RA$ Lösungen liefern untere Schranken für die ganzzahlige Optimallösung. Obere Schranken erhält man durch Heuristiken, \zb Runden der $w_i$ auf zulässige ganze Werte, Beispiel: Sum-Up-Rounding. TODO: Bildchen. Addiere Gewichte solange auf, bis die Summe $\geq 1$ wird und platziere dann eine Messung, fahre mit dem Rest und den weiteren Gewichten fort.

Beobachtung bei numerischen Ergebnissen: Die Lösungen der relaxierten Probleme sind schon vollständig oder beinahe 0-1. Erklärungsansätze:
\bitm
\item Exakte lineare Designs (Pukelsheim, Rieder)
\item Bang-Bang-Prinzip der optimalen Steuerung (Sager 2009)
\eitm
\item Lösung des endlichdimensionalen NLP:

Benutze SQP-Verfahren, siehe Kapitel 7. Benötigte Ableitungen:
\bitm
\item Hessematrix der Lagrangefunktion durch Niedrigrang-Aufdatierungen, \zb BFGS.
\item Jacobi-Matrix der Nebenbedingungen 
\item Gradient der Zielfunktion
\eitm
\begin{align*}
\Phi(C) &= \Phi \l( \bpm I & 0 \epm M^{-1} \bpm I \\ 0 \epm \r) \\
M &= \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm \\
J_1 &= \l( - \frac{\sqrt{w_i}}{\sigma_i} \frac{\partial h_i}{\partial y}{\partial y}{\partial v} (t_i) + \frac{\partial h_i}{\partial v} \r)_{i=1,\cdots,M} \\
J_2 &= \frac{\partial F_2}{\partial y} \frac{\partial y}{\partial v} + \frac{\partial f_2}{\partial v} \\
\end{align*}
Gradient von $\Phi$ nach $\xi$:
\[ \frac{\partial \Phi}{\partial \xi} = \frac{\partial \Phi}{\partial C} \frac{\partial C}{\partial M} \frac{\partial M}{\partial J} \frac{\partial J}{\partial \xi} \quad (11.30) \]
$\frac{\partial \Phi}{\partial C}$: Ableitung von Funktionen auf Matrizen
\begin{align*}
\frac{\partial \trace C}{\partial C} \Delta C :&= \lim_{h\to 0} \frac{\trace(C+h\Delta C) - \trace C}{h} \quad (11.31) \\
&= \trace \Delta C \\
\frac{\partial \det C}{\partial C} \Delta c &= \sum\limits_{i,k} (\det C) (C^{-1})_{ik} \Delta C_{ik} \quad (11.32) \\
\frac{\partial \lambda_{max} (C)}{\partial C} \Delta C &= z^T \Delta C z \\
\end{align*}

Wenn $z$ auf 1 normierter EV zum einfachen EW $\lambda_{max}(C)$ ist.

\begin{align*}
\frac{\partial C}{\partial M} \Delta M &= \bpm I & 0 \epm \frac{\partial M^{-1}}{\partial M} \Delta M \bpm I \\ 0 \epm \\
\end{align*}

\eitm

\msubsection{Lemma 11.12: Ableitung der inversen Matrix}

Sei $A \in R^{n\times n}$ invertierbar und $\Delta A \in \R^{n\times n}$. Dann ist

\[ \frac{\partial A^{-1}}{\partial A} \Delta A = - A^{-1} \Delta A A^{-1} \quad (11.34) \]

Beweis: Sei $X = A^{-1}$. Es gilt: $XA = I$. abgeleitet:

\begin{align*}
\frac{\partial X}{\partial A} \Delta A \cdot A + X \frac{\partial A}{\partial A} \Delta A = 0 \\
\RA \frac{\partial X}{\partial A} \Delta A &= -X \Delta A A^{-1} = -A^{-1} \Delta A A^{-1} \\
\frac{\partial C}{\partial M} \Delta M &= - \bpm I & 0 \epm M^{-1} \Delta M M^{-1} \bpm I \\ 0 \epm \\
\frac{\partial M}{\partial J} \Delta J &= \bpm J_1^T \Delta J_1 + \Delta J_1^T J_1 & \Delta J_2^T \\ \Delta J_2^T & 0 \epm 
\frac{\partial J_1}{\partial \xi}
\end{align*}

ableiten nach $w,q$ benötigt insbesondere $\frac{\partial}{\partial q} \frac{\partial y}{\partial p} =: GAQ$ Lösung einer VDE zweiter Ordnung:

\begin{align*}
\dot y &= f(t,y,p,q) \\
y(t_0) &= y_0(p,q) \quad (11.35) \\
\end{align*}

Ableiten nach $p$: $G := \frac{\partial y_0}{\partial p}$

Ableiten nach $q$: 

\begin{align*}
GAQ :&= \frac{\partial^2 y}{\partial q \partial p} \\
GAQ(t_0) &= \frac{\partial^2 y_0}{\partial q \partial p} \\
G\dot A Q &= \frac{\partial f}{\partial y} GAQ + \l( \frac{\partial^2 f}{\partial y \partial p} GQ + \frac{\partial^2 f}{\partial q \partial y} \r) G + \l( \frac{\partial^2 f}{\partial y \partial p} GQ + \frac{\partial^2 f}{\partial y \partial p} \r) \\
\end{align*}

Ableitung nachz $q$:

\begin{align*}
GQ &= \frac{\partial y}{\partial q} \\
\dot Gq &= \frac{\partial f}{\partial y} GQ + \frac{\partial f}{\partial q} \\
GQ(t_0) &= \frac{\partial y_0}{\partial q}
\end{align*}

Müssen als System für $y,G,GQ, GAQ$ gelöst werden.

TODO: Fehlender Text

\msection{5. Ausnutzung der Mehrfachexperimentstruktur}

\msubsection{Anwendungsbeispiel: Ergebnisse}

Urethan-Reaktion, siehe 11.1

a) Intuitive Versuchsplanung (Chemiker, BASF): Batch-Versuche, isotherm, "`Trennung der Reaktion"', 15 Experimente, 90 Messungen

Konfidenzintervalle nach Parameterschätzung:

\bitm
\item $k_{ref1}$ $\pm 0.93\%$
\item $E_{a1}$ $\pm 0.88\%$
\item $k_{ref2}$ $\pm 0.72\%$
\item $E_{a2}$ $\pm 0.26\%$
\item $k_{ref4}$ $\pm 0.01\%$
\item $DH_2$ $\pm 32.96\%$
\item $KL_2$ $\pm 22.4\%$
\eitm

b) Optimale Versuchsplanung mit VPLAN:

Beschränkungen an die Steuergrößen, maximal 16 Messungen pro Experiment. Tag-Nacht-Schichtrhythmus 80h. Nachts keine Messungen und keine Änderungen der Steuerungen.

$\to$ 2 Experimente sequentiell $\rho$ alle Parameter mit Standardabweichung $< 1\%$

