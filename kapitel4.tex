\chaptr{4. Verallgemeinerte Gauß-Newton-Verfahren}

Problem:

\begin{align*}
\min \frac 12 \|F_1(x)\|_2^2 &\text{ s.\,t. } F_2(x) = 0 \quad (4.1) \\
\text{ mit } F_1\colon D \subset \R^n  &\to \R^{m_1}, \quad F_1 \in C^2(D) \\
F_2\colon D \subset \R^n &\to R^{m_2}, \quad F_2 \in C^2(D) \\
n & \geq m_2 \\
m_1 \geq n - m_2 \\
\text{Jacobi-Matrizen: } J_1 \colon&= \frac{\partial F_1}{\partial x}, \quad J_2 \colon= \frac{\partial F_2}{\partial x}
\end{align*}

\begin{itemize}
\item Unbeschränkter Fall: $\min \frac 12 \|F(X)\|_2^2$
\item Spezialfall: nichtlineare Gleichung: $F(x) = 0$
\end{itemize}

\msection{Algorithmus 4.1 (Verallgemeinertes Gauß-Newton-Verfahren)}

\begin{itemize}
\item Startpunkt $x^0$, $k \colon= 0$
\item Solange ein Abbruchkriterium verletzt ist (z.\,B. $\|\Delta x\| > \eps$):
\begin{itemize}
\item Berechne
\begin{align*}
F_1^k \colon&= F_1(x^k) \\
F_2^k \colon&= F_2(x^k) \\
J_1^k \colon&= J_1(x^k) \\
J_2^k \colon&= J_2(x^k) \\
\end{align*}
\item Löse das lineare Ausgleichsproblem
\[ \min \frac 12 \|F_1^k + J_1^k \Delta x \|_2^2 \text{ s.\,t. } F_2^k + J_2^k \Delta x = 0 (4.2) \]
Lösung: $\Delta x^k$
\item Bestimme eine Schrittweite $\alpha^k \in (0,1]$
\item Iteriere $x^{k+1} \colon= x^k + \alpha^k \Delta x^k$
\end{itemize}
\end{itemize}

"`Newton"': löse iterativ, linearisiere in jeder Iteration.

"`Gauß"': linearisiere innerhalb der Norm, löse das lineare Ausgleichsproblem.

\msubsection{Bemerkung 4.2}

Zur Globalisierung der Konvergenz kann z.\,B. Linesearch verwendet werden. Es ergeben sich gedämpfte Schritte $\alpha^k < 1$. In der Nähe der lösung können Vollschritte $\alpha^k=1$ erwartet werden.

\msubsection{Annahmen 4.3: Regularitätsannahmen}

\begin{itemize}
\item (CQ) "`Constraint Qualification"': $\Rg J_2 = m_2$ ($J_2$ hat vollen Rang, die Nebenbedingungen sind widerspruchsfrei und nicht redundant).
\item (PD) "`Positive Definiteness"':
\[ \Rg \begin{pmatrix} J_1 \\ J_2 \end{pmatrix} = n \quad (4.4)\]
bedeutet: die nicht durch die Nebenbedingungen festgelegten Variablen können aus den experimentellen Daten eindeutig geschätzt werden.
\end{itemize}

\msection{Lemma 4.4}

Gelten (PD) und (CQ), dann gilt:

\begin{itemize}
\item Die Matrix $J_1^T J_1$ ist positiv definit auf $\ker J_2$
\item Die Matrix
\[ \begin{pmatrix} J_1^T J_1 & J_2^T \\ J_2 & 0 \end{pmatrix} \]
ist regulär.
\end{itemize}

Beweis: Übungsaufgabe.

\msection{Lösung der linearen Ausgleichsprobleme}

\msubsection{1. Unbeschränkter Fall}

\[ \min \frac 12 \| F_1 + J_1 \Delta x \|_2^2 \quad (4.5) \]

Es gelte (PD), d.\,h. $J_2$ habe vollen Rang. $\Delta x^*$ ist Lösung von (4.5) genau dann, wenn es das sogenannte Normalgleichungssystem löst:

\[ J_1^T J_1 \Delta x^* + J_1^T F_1 = 0 \]

$\Delta x^*$ ist Minimum genau dann wenn 

\begin{align*}
F_1 + J_1 \Delta x^* &\perp \{ J_1 \Delta x\}\\
\LRA (\Delta x^* J_1^T)(J_1 \Delta x^* + F_1) &= 0 \, \forall \Delta x \\
\LRA \Delta x^* (J_1^T J_1 \Delta x^* + J_1^T F_1) &= 0 \\
\LRA J_1^T J_1 \Delta x^* + J_1^T F_1 &= 0\\
\LRA \Delta x^* &= - (J_1^T J_1)^{-1} J_1^T F_1 \\
&= - J^\dagger F_1 & (4.6)
\end{align*}

$J^\dagger$ heißt Moore-Penrose-Pseudoinverse und erfüllt die vier Moore-Penrose-Axiome:

\begin{itemize}
\item $(J^\dagger J)^T = J^\dagger J$
\item $(J J^\dagger)^T = J J^\dagger$
\item $J J^\dagger J = J$
\item $J^\dagger J J^\dagger = J^\dagger$
\end{itemize}

Umgekehrt: $J^\dagger$ ist durch die vier Axiome eindeutig bestimmt. Die Lösung der Normalengleichung ist eindeutig, wenn (PD) erfüllt ist.

\msubsection{Bemerkung 4.10}

Wenn $\Rg J_1 < n$, also (PD) nicht erfüllt ist, dann ist die Lösung von $\min \frac 12 \|F_1 + J_1 \Delta x \|_2^2$ nicht eindeutig. Dann ist

\begin{align*}
\Delta x &= - J^\dagger F_1
\end{align*}

Die Lösung kleinster euklidischer Norm. $J^\dagger$ erfülle dabei die vier Moore-Penrose-Axiome.

Beweis: Übungsaufgabe % Ansatz: Nehme zwei verschiedene Lösung an, forme eine Lösung in die andere um.

\msubsection{2. Beschränkter Fall}

\msubsubsection{Lemma 4.5}

Gelten (CQ) und (PD) dann gilt

$\Delta x^* \in \R^n$ ist genau dann Lösung von $\min \frac 12 \| F_1 + F_1 \Delta x\|_2^2$, s.\,t. $F_2 + J_2 \Delta x = 0$ (4.6), wenn 

\begin{align*}
\Delta x^* &= - \begin{pmatrix} I & 0 \end{pmatrix} \begin{pmatrix} J_1^T J_1 & J_2^T \\ J_2 & 0 \end{pmatrix}^{-1} \begin{pmatrix} J_1^T & 0 \\ 0 & I \end{pmatrix} \begin{pmatrix} F_1 \\ F_2 \end{pmatrix} & (4.7)
\end{align*}

Sei $\Delta x^*$ Minimum.

\begin{align*}
f(t) \colon&= \frac 12 \| F_1 + J_1 (\Delta x^* + t \Delta y) \|_2^2
\end{align*}

mit $\Delta y$ so dass 

\begin{align*}
F_2 + J_2(\Delta x^* + t \Delta y) &= 0 \\
\RA J_2 \Delta y &= 0 \\
\text{d.\,h. } \Delta y &\in \ker(J_2) \\
0 &= f'(0) \\
&= \Delta y^T ( J_1^T J_1 \Delta x^* + J_1^T F_1) \\
\RA J_1^T J_1 \Delta x^* + J_1^T F_1 & \in (\ker J_2)^\perp = \Image \l(J_2^T\r) \\
\RA \exists! \lambda \text{ mit } J_1^T J_1 \Delta x^* + J_1^T F_1 + J_2^T \lambda &= 0 \\
\text{außerdem } J_2 \Delta x^* + F_2 &= 0 \quad \text{ (KKT-Bedinungen)} \\
\RA \begin{pmatrix} J_1^T J_1 & J_2^T \\ J_2 & 0 \end{pmatrix} \begin{pmatrix} \Delta x^* \\ \lambda \end{pmatrix} &= - \begin{pmatrix} J_1^T & 0 \\ 0 & I \end{pmatrix} \begin{pmatrix} F_1 \\ F_2 \end{pmatrix} & (4.8) \\
\RA \Delta x^* &= - \underbrace{\begin{pmatrix} I & 0 \end{pmatrix} \begin{pmatrix} J_1^T J_1 & J_2^T \\ J_2 & 0 \end{pmatrix}^{-1} \begin{pmatrix} J_1^T & 0 \\ 0 & I \end{pmatrix}}_{=\colon J^+ \quad (4.9)} \begin{pmatrix} F_1 \\ F_2 \end{pmatrix}  \\
\Delta x^* &= - J^+ F \\
\end{align*}

Gelte umgekehrt (4.8) für $\Delta x^*$ und $\lambda$. Dann folgt $f'(0) = 0$ für alle $\Delta y \in \ker(J_2)$. Aus (PD) folgt, dass $\Delta x^*$ ein Minimum ist.

\msubsubsection{Definition 4.6: Verallgemeinerte Inverse}

Der Lösungsoperator $J^+$ von (4.8) heißt verallgemeinerte Inverse von

\[ J = \begin{pmatrix} J_1 \\ J_2 \end{pmatrix} \]

\msubsection{Lemma 4.7}

$J^+$ erfüllt das Moore-Penrose-Axiom $J^+ J J^+ = J^+$.

Beweis: Setz

\begin{align*}
F = \begin{pmatrix} F_1 \\ F_2 \end{pmatrix} &= \begin{pmatrix} J_1 J^+ y \\ J_2 J^+ y \end{pmatrix}
\end{align*}

in (4.6) ein:

\[ \min \frac 12 \| F_1 + J_1 \Delta x\|_2^2 \text{ s.\,t. } F_2+ J_2 \Delta x = 0 \]

Erste Lösung:

\begin{align*}
\Delta x_1 &= - J^+ F \\
= - J^+ (J J^+ y) \\
\end{align*}

Zweite Lösung:

\begin{align*}
\Delta x_2 &= - J^+ y\\
\text{NR: } \underbrace{J_1 J^+ y}_{=F_1} - J_1 J^+ y &= 0 \\
\underbrace{J_2 J^+ y}_{=F_2} - J_2 J^+ y &= 0
\end{align*}

Wegen der Eindeutigkeit der Lösung ist $\Delta x_1 = \Delta x_2$. $-J^+ J J^+ y = -J^+y$ für beliebige $y$, $J^+ J J^+ = J^+$.

\msubsubsection{Lemma 4.8}

$J^+$ Erfüllt die Moore-Penrose-Axiome 1,2 und 4. Beweis: Übungsaufgabe.

\msubsubsection{Bemerkung 4.9}

Die Moore-Penrose-Pseudoinverse für den beschränkten Fall würde das folgende Problem lösen:

\begin{align*}
\min_{\Delta x} \frac 12 \l\| \begin{pmatrix} J_1 \\ J_2 \end{pmatrix} \Delta x + \begin{pmatrix} F_1 \\ F_2 \end{pmatrix} \r\|_2^2
\end{align*}

Das ist nicht äquivalent zu 

\begin{align*}
\min_{\Delta x} \frac 12 \| F_1 + J_1 \Delta x \|_2^2 \\
\text{s.\,t. } F_2 + J_2 \Delta x = 0
\end{align*}

\msubsection{Numerische Lösung}

Man stellt nicht $J^+$ oder $J^\dagger$ auf, sondern zerlegt $J_1$ und $J_2$ geeignet.

\msubsubsection{1. Unbeschränkter Fall}

\begin{align*}
J_1^T J_1 \Delta x &= -J_1^T F_1
\end{align*}

\begin{itemize}
\item 1. Variante: QR-Zerlegung von $J_1 = QR = \overline Q \overline R$
\begin{align*}
J_1^T J_1 \Delta x &= \overline R^T \underbrace{\overline Q^T \overline Q}_{=I} \overline R \Delta x \\
&= - \overline R^T \overline Q^T F_1 \\
\Delta x &= -\overline R^{-1} \overline Q^T F_1 \\
J^\dagger &= \overline R^{-1} \overline Q^T \\
\end{align*}
\item 2. Variante: Singulärwertzerlegung von $J_1$:
\begin{align*}
J_1 &= U \Sigma V^T
\end{align*}
mit $U,V$ orthogonal und $\Sigma$ Diagonalmatrix mit Singulärwerten von $J_1$ auf der Diagonalen.
\begin{align*}
\Delta x &= - V \begin{pmatrix} \sigma_1^{-1} \\ & \ddots & & 0 \\ & & \sigma_n^{-1} \end{pmatrix} U^T F_1 \\
J^\dagger &= V \begin{pmatrix} \sigma_1^{-1} \\ & \ddots & & 0 \\ & & \sigma_n^{-1} \end{pmatrix} U^T \\
\end{align*}
\end{itemize}

\msubsubsection{Beschränkter Fall}

\begin{itemize}
\item 1. Variante ("`Bildraumvariante"'): $\Delta x$ ist Lösung von 
\[ \begin{pmatrix} J_q^T J_1 & J_2^T \\ J_2 & 0 \end{pmatrix} \begin{pmatrix} \Delta x \\ \lambda \end{pmatrix} = - \begin{pmatrix} J_1^T F_1 \\ F_2 \end{pmatrix} \]
Diese Matrix ist symmetrisch aber indefinit, also wende Gaußsche LR-Zerlegung an
\item 2. Variante ("`Nullraumvariante"'): % "Man kann das Optimierungsproblem nicht oft genug hinschreiben"
\[ \min_{\Delta x} \frac 12 \| F_1 + J_1 \Delta x\|_2^2 \text{ (4.10) s.\,t. } F_2 + J_2 \Delta x = 0 \text{ (4.11)} \]
1. Bestimme Lösungsmenge von (4.11) durch LR-Zerlegung von $P_2 J_2$:
\[ P_2 J_2 = L_2 \begin{pmatrix} R_2 & D_2 \end{pmatrix} = \begin{pmatrix} P_2 J_{21} & P_2 J_{22} \end{pmatrix} \quad (4.12)\]
Mit $R_2$ reguläre obere Dreiecksmatrix, $L_2$ normierte untere Dreiecksmatrix, $P_2$ Permutationsmatrix zur Zeilenpivotierung. Vorraussetzung ist (CQ). Spalte $\Delta x$ auf: 
\begin{align*}
\Delta x &= \begin{pmatrix}\underbrace{ \Delta y}_{m_2} & \underbrace{\Delta z}_{n-m_2} \end{pmatrix} \\
P_2 J_2 \Delta x &= L_2( R_2 \Delta y + D_2 \Delta z) = - P_2 F_2 \\
\RA \Delta y &= - R_2^{-1} ( D_2 \Delta z + L_2^{-1} P_2 F_2) \quad (4.13) \\
\end{align*}
2. Minimiere (4.10) auf der Lösungsmenge von (4.11). Spalte $J_1$ analog auf.
\begin{align*}
J_1 &= \begin{pmatrix}\underbrace{ J_{11}}_{m_1 \times m_2} & \underbrace{J_{12}}_{m_1 \times (n-m_2)} \end{pmatrix} \\
\text{Einsetzen: } \frac 12 \| F_1 + J_1 \Delta x \|_2^2 &= \frac 12 \|F_1 + J_{11} \Delta y + J_{12} \Delta z \|_2^2 \\
&= \frac 12 \|\underbrace{F_1 + J_{11} R_2^{-1} L_2^{-1} P_2 F_2}_{=: b} + (\underbrace{J_{12} - J_{11} R_2^{-1} D_2}_{=\colon B}) \Delta z \|_2^2\\
&= \frac 12 \|b + B \Delta z \|_2^2 \\
\end{align*}
QR-Zerlegung von $B$:
\begin{align*}
B &= Q_1 R_1 = \begin{pmatrix} \overline Q_1 & \hat Q_1 \end{pmatrix} \begin{pmatrix} \overline R_1 \\ 0 \end{pmatrix} = \overline Q_1 \overline R_1 \quad (4.15) \\
B^T B \Delta z &= \overline R_1^T \overline Q_1^T \overline Q_1 \overline R_1 \Delta z = -B^T b = -\overline R_1^T \overline Q_1^T b \\
\Delta z &= - \overline R_1^{-1} \overline Q_1^T b \quad (4.16)\\
\text{Einsetzen: } \Delta y &= -R_2^{-1} ( D_2 \Delta z + L_2^{-1} P_1 F_2 \\
\end{align*}
Formale Darstellung:
\begin{align*}
\bpm P_2 J_2 \\ \cdots \\ J_1 \epm &= \bpm P_1 J_{21} & \vdots & P_2 J_{22} \\ \cdots & & \cdots \\ J_11 & \vdots & J_{12} \epm \\
&= \bpm L_2 & \vdots & 0 \\ \cdots & & \cdots \\ L_1 & \vdots & I \epm \bpm R_2 & \vdots & D_2 \\ \cdots & & \cdots \\ 0 & \vdots & B \epm \\
&= \underbrace{ \bpm L_2 & \vdots & 0 \\ \cdots && \cdots \\ L_1 & \vdots & Q_1 \epm}_{=: T} \bpm R_2 & \vdots & D_2 \\ \cdots && \cdots \\ 0 & \vdots & R_1 \epm \quad (4.17) \\
L_1 :&= J_{11} R_2^{-1} \\
\RA J_{12} &= L_1 D_2 + B = L_1 D_2 + Q_1 R_1 \\
\end{align*}
Rechte Seite entsprechend:
\begin{align*}
- \bpm P_2 F_2 \\ F_1 \epm &= T \bpm -L_2^{-1} P_2 F_2 \\ - Q_1^T b \epm \quad (4.18) \\
\end{align*}
nachrechnen:
\begin{align*}
-L_2 L_2^{-1} P_2 F_2 &= - P_2 F_2 \\
-L_1 L_2^{-1} P_2 F_2 - \underbrace{Q_1 Q_1^T}_{=I} (\underbrace{F_1 - \overbrace{J_{11} R_2^{-1}}^{= L_1} L_2^{-1} P_2 F_2  }_{b}) = - F_1
\end{align*}
Lösen:
\begin{align*}
R_2 \Delta y + D_2 \Delta z &= - L_2^{-1} P_2 F_2 \\
\ov R_1 \Delta z &= -\ov Q_1^T b \\
\bpm R_2 & D_2 \\ 0 & \hat R_1 \epm \bpm \Delta y \\ \Delta z \epm &= - T^{-1} \bpm P_2 F_2 \\ F_1 \epm \\
&= - T^{-1} \bpm 0 & P_2 \\ I & 0 \epm \bpm F_1 \\ F_2 \epm \\
\Delta x &= - \underbrace{ \l[ \bpm R_2 & D_2 \\ 0 & \ov R_1 \epm^{-1} \vdots 0 \r] T1^{-1} \bpm 0 & P_2 \\ I & 0 \epm}_{= J^+} \underbrace{\bpm F_1 \\ F_2 \epm}_{F} \quad (4.19) \\
&= - J^+ F
\end{align*}
\end{itemize}

\msubsection{Lemma 4.11 (Berechnung der adjungierten }

\begin{align*}
\lambda &= - P_2 L_2^{-T} R_2^{-T} J_{11}^T (J_1 \Delta x + F_1) \\
&= - P_2 L_2^{-T} L_1^T (J_1 \Delta x + F_1) \quad (4.20) \\
\end{align*}

Beweis: $\lambda$ erfüllt eindeutig die Gleichungen

\begin{align*}
J_1^T J_1 \Delta x + J_2^T \lambda &= - J_1^T F_1 \quad (\lambda \in \R^{m_2} \\
\text{bzw. } J_1^T (J_1 \Delta x + F_1) &= -J_2^T \lambda \\
\end{align*}

Es reicht, $m_2$ dieser Gleichungen zu betrachten:

\begin{align*}
J_{11}^T (J_1 \Delta x + F_1) &= - J_{21}^T \lambda = - R_2^T L_2^T P_2^{-1} \lambda \\
\RA \lambda &= - P_2 L_2^{-T} \underbrace{R_2^{-T} J_{11}^T}_{=L_1} (J_1 \Delta x + F_1)
\end{align*}

\msubsection{3. Variante: Stoer 1979}

QR-Zerlegung von $J_2^T$: $J_2 = L_2 Q_2$. Transformation von rechts:

\begin{align*}
Q_1 \bpm J_2 \\ \cdots \\ J_1 \epm Q_2^T &= TODO!!!!!
\end{align*}

Rechte Seite transformieren mit $Q_1$, $\Delta \tilde y$, $\Delta \tilde z$ ausrechnen:

\[ \Delta x = Q_2^T \bpm \Delta \tilde y \\ \Delta \tilde z \epm \]

\msubsection{Anwendung auf die Mehrzielmethode}

TODO!!!!! Große Matrix






























