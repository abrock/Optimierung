% Wiederholen schadet nicht und vielleicht verstehen Sie es dieses Mal

\chaptr{5. Lokale Konvergenz von Newton-Typ-Verfahren}

Newton-Typ-Verfahren:

\bitm
\item Löse iterativ mit einem guten Startwert $x_0 \in \R^n$
\item Löse in jeder Iteration ein linearisiertes Problem
\item Wende eine Globalisierungsstrategie an
\eitm

\[ F\colon \R^n \to \R^m \quad M(x)\colon \text{ Lösungsoperator des linearen Problems} \]

\msection{Algorithmus 5.1 (Newton-Typ-Verfahren)}

\bitm
\item Startwert $x^0$, $k := 0$
\item Solange ein geeignetes Abbruchkriterium verletzt ist:
\bitm
	\item Berechne $\Delta x^k := - M(x^k) F(x^k)$ (5.1)
	\item Berechne $\alpha^k$ auf einer Globalisierungsstrategie
	\item Iteriere $x^{k+1} := x^k + \alpha^k \Delta x^k$ (5.2)
\eitm
\eitm

Kann angewendet werden:

\bitm
\item Zur Bestimmung von Nullstellen von $F(\cdot)$, $m=n$ $\RA$ Newton-Verfahren oder Quasi-Newton-Verfahren
\item Insbesondere zur Bestimmung von Nullstellen von $\Delta L(x,\lambda) = 0$ (notwendige Optimalitätsbedingung) $\RA$ SQP-Verfahren.
\item Zur Bestimmung von Lösungen unbeschränkter nichtlinearer Ausgleichsprobleme $\min \frac 12 \|F(x)\|_2^2$, $m \geq n$
\item Zur Bestimmung von Lösungen beschränkter nichtlinearer Ausgleichsprobleme $\min \frac 12 \|F_1(x)\| \text{ s.\,t. } F_2(x) = 0$, $m = m_1+m_2 \geq n$, $m_2 \leq n$ $\RA$ Verallgemeinertes Gauß-Newton-Verfahren.
\eitm

\[ \text{Sei } J := \frac{\partial F}{\partial x} \quad (5.3) \text{ Jacobimatrix} \]

\msubsection{Bemerkung 5.2}

\bitm
\item Bei Newton-Verfahren ist $M(x) = J(x)^{-1}$ die Inverse von $J$.
\item bei Quasi-Newton-Verfahren ist $M(x) \cong J(x)^{-1}$
\item Bei SQP-Verfahren ist $M(x, \lambda) = \nabla_{x,\lambda}^2 L(x, \lambda)^{-1}$ oder 
$M(x, \lambda) \cong \nabla_{x,\lambda}^2 L(x, \lambda)^{-1}$
\item Bei Gauß-Newton-Verfahren ist $M(x) = (J(x)^T J(x))^{-1} J(x)^T$ die Moore-Penrose-Pseudoinverse.
\item Bei Verallgemeinerten Gauß-Newton-Verfahren ist \[ M(x) = \bpm I & 0 \epm \bpm J_1(x)^T J_1(x) & J_2(x)^T \\ J_2(x) & 0 \epm^{-1} \bpm J_1(x)^T 0 \\ 0 & I \epm \] die verallgemeinerte Inverse von $J$.
\eitm

\msection{Satz 5.3: Lokaler Kontraktionssatz (Bock 1987)} % Enorm wichtig

\[ \text{Sei } F\colon D \subset \R^n \to \R^m, \quad F \in \C^1(D,\R^m), \quad J := \frac{\partial F}{\partial x} \]

Für alle $x,y \in D$ mit $y-x = -M(x) F(x)$ und $\theta \in [0,1]$ gelte:

\bitm
\item Es existiert ein $\omega < \infty$ so dass $\| M(y) (J(x+\theta(y-x)) - J(x)) (y-x) \| \leq \omega \theta \|x-y\|^2$ (5.4)
\item Es existiert ein $\kappa(x) \leq \kappa < 1$, so dass $\| M(y) R(x) \| \leq \kappa(x) \|y-x\|$ (5.5) für das Residuum $R(x) := F(x) - J(x)M(x)F(x)$
\item Sei $x_0 \in D$ gegeben mit $\Delta x^j := -M(x^j) F(x^j)$, $\delta_j := \kappa + \frac \omega 2 \| \Delta x^j \|$, $\delta_0 = \kappa + \frac \omega 2 \|\Delta x^0 \| < 1$ (5.6)
\item \[ D^0 := \l\{ z : \|z-z_0\| \leq \frac{\|\Delta x^0\|}{1-\delta 0} \r\} \subset D \]
\eitm

Dann gilt:

\bitm
\item a) Die Iterierten $x^{j+1} = y^j + \Delta x^j$ sind wohldefiniert und bleiben in $D^0$.
\item b) Es existiert ein $x^* \in D^0$ so dass $x^j \to x^*$ ($j \to \infty$)
\item c) \[ \| x^{j+k} - x^*\| \leq \frac{\Delta x^0}{1-s_j} s_j^k \text{ "`hoch k"' (a-priori-Abschätzung)} \]
\item d) \[ \| \Delta x^{j+1} \| \leq s_j \| \Delta x^j \| = \kappa \| \Delta x^j \| + \frac \omega 2 \| \Delta x^j\|^2 \text{ (5.7)} \]
\eitm

Beweis:

\begin{align*}
\| \Delta x^{j+1} \| &= \| M(x^{j+1}) F(x^{j+1}) \| =: \| M^{j+1} F^{j+1} \| \\
&= \| M^{j+1} (F^{j+1} - F^j - J^j \Delta x^j) + M^{j+1} R^j \| \\
& \leq \| M^{j+1} \l( \int\limits_0^1 J(x^j + t \Delta x^j) \Delta x^j \dd t - \int\limits_0^1 J^j \Delta x^0 \dd t \r)\| + \|M^{j+1} R^j \| \\
&\leq \int\limits_0^1 \| M^{j+1} (J(x^j + t \Delta x^j) - J^j) \Delta x^j \| \dd t + \|M^{j+1} R^j \| + \kappa \| \Delta x^j \| \\
&\leq \int\limits_0^1 \omega t \| \Delta x^j \|^2 \dd t + \kappa \|\Delta x^j \| \\
&= \frac \omega 2 \|\Delta x^j\|^2 + \kappa \| \Delta x^j \| \\
&= \delta_j \| \Delta x^j \| \RA d) \\
\end{align*}

Zeige: $(\delta_j)_{j \in \N}$, $(\|\Delta x^j\|)_{j \in \N}$ sind monoton fallend:

Induktion: 

\begin{align*}
\delta_j - \delta_{j+1} &= \frac \omega 2(\|\Delta x^j\| - \|\Delta x^{j+1}\|) \\
&\geq \frac \omega 2 (\|\Delta x^j\|-\delta_j \|\Delta x^j\|) \\
&\geq \frac \omega 2 \|\Delta x^j\| (1-\delta_j) > 0\\
\| \Delta x^{k+k} \| &\leq \delta_{j+k} \| \Delta x^{j+k-1} \| \\
& \leq \delta_{j+k-1} \cdots \delta_j \|\Delta x_j\| \\
& \leq \delta_j^k \|\Delta x^j\| \\
\| x^{j+2} - x^0 \| & \leq \|\Delta x^j+1\| + \cdots + \|\Delta x^0\| \\
&\leq ( \delta_0^{j+1} + \cdots + \delta s_0^1) \|\Delta x^0\| \\
&\leq \frac{1}{1-\delta_0} \|\Delta x^0\| \RA a)
\end{align*}


$(x^j)_{j \in \N}$ ist Cauchy-Folge:

\begin{align*}
\| x^{i+j+1} - x^i \| & \leq \sum_{k=0}^j \| \Delta x^{i+k} \\
&\leq \sum\limits_{k=0}^j \delta_j^k \| \Delta x^i\| \\
&\leq \sum\limits_{k=0}^i \delta_0^{i+k} \|\Delta x^0\| \\
&\leq \delta_0^i \l(\sum\limits_{k=0}^\infty \delta_0^k \r) \| \Delta x^0 \| \\
&= \delta_0^i \frac{\|\Delta x^0\|}{1-\delta_0} \to 0 (i \to \infty \\
& \RA x^j \to x^* \\
D \text{ kompakt } & \RA x^* \in D^0 \RA b)
\end{align*}

Beweis:

\begin{align*}
\|x^j+k+i-x^{j+k}\| & \leq \frac{\|\Delta x^{j+k}\|}{1-\delta_{j+k}} \\
&\leq \delta_j^k \frac{\|\Delta x^j\|}{1-\delta_j}
\end{align*}

$\forall i \geq 0$ also auch für $i \to \infty$:

\begin{align*}
\|x^* - x^{j+k} \| & \leq \delta_j^k \frac{\|\Delta x^j\|}{1-\delta_j} \RA c) 
\end{align*}

\msubsection{Korollar 5.4}

Für $F(x)=0$, $F\colon \R^n \to \R^m$ konvergiert das Newton-Verfahren mit $M(x) = J(x)^{-1}$ lokal quadratisch.

Beweis:

\begin{align*}
\| M^{j+1} R^j \| &= \| (J^{j+1})^{-1} (I - J^j (J^j)^{-1}) F^j\| = 0 \RA \kappa = 0 \\
\|\Delta x^{j+1} \| & \leq \frac \omega 2 \| \Delta x^j \|^2 \\
\|x^{j+1} - x^* \| &= \|x^j - x^* + \Delta x^j \| = \| M^j ( J^j(x^j-x^*) - (F^j - F^*)) \| \quad \text{ (*): Im Lösungspunkt}\\
&= \l\| M^j \int\limits_0^1 (J^j-J(x^j + t(x^j-x^*))) (x^j-x^*) \dd t \r\| \quad \text{(HDI)} \\
&\leq \underbrace{\|M^j J^*\|}_{\leq \Gamma} \int\limits_0^1 \| M^*(J^j-J(x^j+t(x^j-x^*)))(x^j-x^*) \| \dd t \\
&\leq \Gamma \int\limits_0^1 \omega t \|x^j - x^*\|^2 \dd t \\
&= \Gamma \frac \omega 2 \|x^j - x^*\|^2 \\
\end{align*}

% Tafel geht kaputt,... "Das ist mir jetzt eigentlich egal"

\msubsection{Bemerkung 5.5: Quasi-Newton-Verfahren}

Für näherungsweise Newton-Verfahren ("`Quasi-Newton-Verfahren"') ist $x^{j+1} = x^j - M(x^j) F(x^j)$ mit $M(x^j) \cong J(x^j)^{-1}$ $\kappa > 0$, zur Konvergenz muss $\kappa < 1$ sein:

\begin{align*}
\|M^{j+1} R^j\| &= \|M^{j+1} ((M^j)^{-1} - J^j) M^j F^j \| \\
&= \| M^{j+1} ((M^j)^{-1} - J^j) (x^{j+1} - x^j) \| \\
&\leq \kappa \|<x^{j+1} x^j\| \\
\end{align*}

Notwendig für Konvergenz ist also

\begin{align*}
\|M^j\| & \leq \gamma \text{ und } \|(M^j)^{-1}-J^j\| \text{ klein} 
\end{align*}

\msubsection{Satz 5.6 (Dennis-Mor\'e}

Sei $F\colon \R^n \to \R^n$ stetig differenzierbar. Betrachte die Iteration $x^{j+1} = x^j + \Delta x^j$ und sei $\Delta x^j$ gegeben durch $\Delta x^j = -M(x^j) F(x^j)$. Wir nehmen an, dass die Folge der $x^j$ gegen einen Punkt $x^j$ mit $F(x^j) = 0$ konvergiert mit $J(x^*)$ regulär. Dann konvergiert $(x^j)_{j\in \N}$ Q-superlinear gegen $x^*$, d.\,h. $\lim_{j\to \infty} \frac{\|x^{j+1} - x^*\|}{\|x^j-x^*\|} = 0$ genau dann wenn

\[ \lim_{j \to \infty} \frac{\| (M(x^j)^{-1} - J(x^*)) \Delta x^j \|}{\|\Delta x^j\|} = 0 \quad (5.8) \]

Beweis: siehe Vorlesung Algorithmische Optimierung 1.

\msubsection{Varianten von Quasi-Newton-Verfahren}

\[ \Delta x^j = -M(x^j) F(x^j), \quad M(x^j) \cong J(x^j)^{-1} \]

\bitm
\item Berechne $J(x^j)$ durch Differenzenquotienten
\item Halte $M(x^j)$ fest
\bitm
	\item für alle Iterationen: $M(x^j) = J(x_0)^{-1}$
	\item für einige Iterationen: $M(x^j) = J(x^{\ov j})^{-1}$ solange $\frac{\Delta x^{j+1}}{\|\Delta x^j\|} \leq \delta$ z.\,B. $\delta = \frac 14$, danach neues $\overline j = j$ 
\eitm
\item Nähere $J(x^j)$ bzw. $M(x^j)$ durch Update-Formeln aus $J(x^{j-1})$ bzw. $M(x^{j-1})$ an, siehe unten.
\eitm

\msubsection{Bemerkung 5.7 (Bedeutung von $\omega$)}

\[ \| M(y) (J(x+t(y-x)) - J(x)) (x-y) \| \leq t \omega \|y-x\|^2 \]

\bitm
\item Wenn $M(y)$ in einer Umgebung von $x^*$ beschränkt ist: $\|M(y)\| \leq \gamma$, $\gamma < \infty$ und
\item $J$ eine Lippschitz-Bedingung erfüllt: $\|(J(x+t(x-y))-J(x)) (y-x)\| \leq \beta t \|y-x\|^2$, $\beta < \infty$, dann ist $\omega = \gamma \beta < \infty$ in einer Umgebung von $x^*$
\eitm

$\omega$ kann sehr groß werden wenn

\bitm
\item $\|M\|$ sehr groß ist, d.\,h. $J$ ist fast singulär.
\item die erste Ableitung von $J$ bzw. die zweite Ableitung von $F$ groß ist, d.\,h. das Problem ist sehr nichtlinear.
\eitm

\msubsection{Anwendung auf (verallgemeinerte) Gauß-Newton-Verfahren:}

\[ M(x) = J^+(x) = \bpm I & 0 \epm \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm^{-1} \bpm J_1^T & 0 \\ 0 & I \epm \]

Es gelte (CQ) und (PD). $\|M\|$ ist groß, wenn

\[ \bpm J_1^T J_1 & J_2^T \\ J_2 & 0 \epm \]

fast singulär ist.

Residuum:

\begin{align*}
R(x) &= F(x) - J(x) M(x) F(x) \\
&= F(x) + J(x)\Delta x
\end{align*}

Im Lösungspunkt: $\Delta x^* = 0$: $R(x^*) = F(x^*)$,

\begin{align*}
\| R(x^*) \| &= \| F_1(x^*) \|, \quad \text{ da } F_2(x^*) = 0
\end{align*}

\msubsection{Bemerkung 5.8: Bedeutung von $\kappa$} 

Wenn $M(x)$ stetig differenzierbar ist gilt in $D^0$:

\begin{align*}
\|M(y) - M(x)\| & \leq L \|y-x\| \text{ und } \\
\|R(x)\| &= \|F(x) - J(x)M(x) F(x) \| \leq \rho \\
\end{align*}

Außerdem ist $M(x) R(x) = (\underbrace{M(x) - M(x)J(x) M(x)}_{= 0 \text{ (4. Moore-Penrose-Axiom)}} F(x) = 0$

\begin{align*}
\| M(y) R(x) \| &= \| (M(y)-M(x)) R(x) \| \leq \rho L \|y-x\| \\
\kappa &= \rho L 
\end{align*}

$\kappa < 1$ falls 

\bitm
\item das Residuum $R$ klein ist.
\item $M$ eine Lippschitz-Bedingung mit kleinem $L$ erfüllt, d.\,h. falls die erste Ableitung von $M$ klein ist.
\eitm

\msubsection{Bemerkung 5.9}

Seien $\|M\|$ und $\|M^*\|$ beschränkt, dann ist $\omega$ ein Maß für die Nichtlinearität des Problems und $\kappa$ ein Maß für die Inkompatibilität zwischen Modell und Daten. Probleme mit $\kappa < 1$ heißen Kleine-Residuen-Probleme, Probleme mit $\kappa > 1$ heißen Große-Residuen-Probleme. Anwendung des Kontraktionssatzes:

\msubsection{Korollar 5.10}

Wenn der Startwert $x^0$ nahe der Lösung $x^*$ gewählt wird, d.\,h. wenn $\kappa + \frac \omega 2 \|\Delta x^0\| < 1$ dann konvergiert das (verallgemeinerte) Gauß-Newton-Verfahren  und die Konvergenz ist linear:

\[ \|\Delta x^{k+1}\| \leq \kappa \|\Delta x^k\| + \frac \omega 2 \| \Delta x^k\|^2 \]

Notwendig dafür ist $\kappa < 1$. Wenn $\kappa > 1$ ist konvergiert das Verallgemeinerte Gauß-Newton-Verfahren nicht.

Warum sollte man nichtlineare Ausgleichsprobleme nicht mit den Newton-(SQP)-Verfahren lösen?

Betrachte den unbeschränkten Fall: $\min \frac 12 \|F(x)\|_2^2 = \frac 12 F(x)^T F(x) =: f(x)$. Optimalitätsbedingung:

\begin{align*}
\nabla f(x) &= J^T(x) F(x) =: g(x) = 0 \\
\text{Hessematrix: } H_{f}(x) &= \nabla^2 f(x) = \underbrace{J^T(x) J(x)}_{=: B(x)} + \underbrace{ \sum\limits_{i=1}^n F_i(x) \frac{\partial J_i}{\partial x} (x) }_{=: E(x)} =: H_f(x)
\end{align*}

Newton-Typ-Verfahren: $x^{k+1} = x^k + \alpha^k \Delta x^k$. Schreibweisen: $F := F(x^k)$, $J := J(x^k)$, $f := f(x^k)$, $g := g(x^k)$, $H := H(x^k)$, $B := B(x^k)$, $E := E(x^k)$

\msection{Newton-Verfahren für die nichtlineare Gleichung $\nabla f(x) = 0$}

$\Delta x^k$ löst $\nabla f + \nabla^2 f \Delta x = 0$

\[ \RA \Delta x^k = - (\nabla^2 f)^{-1} \nabla f = -(B+E)^{-1} J^T F = -Mg \quad (M = (B+E)^{-1} \]

Daraus folgt lokal quadratische Konvergenz.

\msection{Gauß-Newton-Verfahren für $\min \frac 12 \|F(x)\|_2^2$}

$\Delta x^k$ löst $\min \frac 12 \| F + J \Delta x\|_2^2 = \frac 12 F^T f + F^T J \Delta x + \frac 12 \Delta x^T J J^T \Delta x$ bzw. $J^T J \Delta x + J^T f = 0$, siehe auch (4.6). $\RA \Delta x^k = -(J^T J)^{-1} J^T F = -Mg$ (5.10) mit $M = (J^T J)^{-1} = B^{-1}$ (5.11)

\msection{Newton-Verfahren für $\min \frac 12 \|F(x)\|_2^2$}

\begin{align*}
\nabla f + \nabla^2 f \Delta x &= 0 \\
\Delta x^k &= -(\nabla^2 f)^{-1} \nabla f \\
&= -(B+E)^{-1} J^T F \\
M &= (B+E)^{-1} & (5.13)
\end{align*}

\msubsection{Bemerkung 5.11}

Die Hessematrix

\begin{align*}
H(x) &= \nabla^2 f(x) \\
&= J^T(x) J(x) + \sum\limits_{i=1}^n F_j \frac{\partial J_i}{\partial x} (x) & (5.9)
\end{align*}

ist die Summe aus

\bitm
\item dem Gauß-Newton-Anteil $B(x) = J^T(x) J(x)$ (deterministischer Anteil) und
\item dem Anteil $E(x) = \sum_{i=1}^n F_j \frac{\partial J_i}{\partial x} (x)$, der von den zweiten Ableitungen $\frac{\partial J_i}{\partial x} (x)$ und vom Residuum $F_i(x)$, $i=1,\cdots,M$ d.\,h. von den zufallsbehafteten Messfehlern abhängt. $E(x)$ ist der Zufallsanteil der Hessematrix.
\eitm

\msection{Satz 5.12: Kleine-Residuen-Probleme}

Äquivalent zu $\kappa < 1$ ist $\rho (B(x^*)1{-1} E(x^*)) < 1$ (5.14), $\rho: $ Spektralradius.

Beweis:

\begin{align*}
M &= J^\dagger \\
&= (J^T j)^{-1} J \\
R &= F - J J^\dagger F \\
y-x &= - J^\dagger(x) F(x) \\
J^\dagger(y) R(x) &= \underbrace{J^\dagger(x) R(x)}_{=0} + \l( \l. \frac{\partial J^\dagger}{\partial y} (y)\r|_{y=x} (y-x) \r) R(x) + \mathcal O(\|y-x\|^2) \\
&= ((J(x)^T J(x))^{-1} \l( \l. \frac{\partial J(y)^T}{\partial y} y\r|_{y=x} (y-x) \r) R(x) + \l( \l. \frac{\partial J(y)^T J(y)}{\partial y} y\r|_{y=x} (y-x) \r) \underbrace{J^T(x) R(x)}_{=0} \\ &+ \mathcal O(\|y-x\|^2) \\
&= B(x)^{-1} \l( \l. \frac{\partial J(y)^T}{\partial y} \r|_{y=x} (y-x) \r) F(x) - B(x)^{-1} \l( \l. \frac{\partial J(y)^T}{\partial y} \r|_{y=x} (y-x) \r)J(x) \underbrace{J^\dagger(x) F(x)}_{=-(y-x)} \\ &+ \mathcal O(\|y-x\|^2) \\
&= B(x)^{-1} E(x) (y-x) + \mathcal O(\|y-x\|^2) \\
\end{align*}

Sei $\rho(B(x^*)1{-1} E(x^*)) =: \kappa_1 < 1$. Wähle eine Umgebung von $x^*$ und eine Norm, so dass $\| B(x)^{-1} E(x) \| \leq \kappa_2 < 1$ für alle $x$ aus dieser Umgebung. Verkleinere die Umgebung evtl., so dass 

\[ \mathcal O(\|y-x\|^2) \leq \frac{1-\kappa_2}{2} \|y-x\| \]

für alle $x,y$ aus dieser Umgebung. Dann ist

\begin{align*}
\| J^\dagger(y) R(x)\| &= \| B(x)^{-1} E(x) (y-x) + \mathcal O(\|y-x\|^2) \| \\
& \leq \kappa_2 \|y-x\| + \frac{1-\kappa_2}{2} \|y-x\| \\
&= \frac{1+\kappa_2}{2} \|y-x\| \\
&=: \kappa\|y-x\| \\
\kappa :&= \frac{1+\kappa}{2} < 1
\end{align*}

Sei umgekehrt die $\kappa$-Bedingung (5.5) erfüllt, d.\,h. $\exists \kappa < 1$ so dass

\[ \| J^\dagger(y) R(x) \| = \| B(x)^{-1} E(x) (y-x) + \mathcal O(\|y-x\|^2) \leq \kappa \|y-x\| \]

Dann ist

\[ \| B(x)^{-1} E(x) (y-x)\| - \mathcal O(\|y-x\|^2) \leq \kappa \|y-x\| \]

Mache die Umgebung so klein, dass

\[ \mathcal O(\|y-x\|^2) \leq \frac{1+\kappa}{2} \|y-x\| \]

Dann ist

\begin{align*}
\| B(x)^{-1} E(x) (y-x) \| & \leq \frac{1+\kappa}{2} \|y-x\| \\
&=: \kappa_1 \|y-x\| \\
\kappa_1 :&= \frac{1+\kappa}{2} < 1 \\
\end{align*}

Daher ist

\[ \| B(x)^{-1} E(x) \| \leq \kappa_1 < 1 \]

nach Definition der Norm für Matrizen. Dann ist auch

\[ \rho( B(x)^{-1} E(x)) < 1 \]

Anders ausgedrückt: Bei Kleine-Residuen-Problemen wird der Zufallsanteil $E$ der Hesse-Matrix $H$ relativ beschränkt durch den deterministischen Anteil $B(x)$.

\msection{Satz 5.13}

Sei $x^*$ ein statischer Punkt des Gauß-Newton-Verfahrens, d.\,h. $J(x^*)^T F(x^*) = 0$ und $\rho (B(x^*)^{-1} E(x^*)) < 1$. Dann ist $H(x^*)$ positiv definit.

Beweis: $J(x^*)$ hat vollen Rang $\RA$ $B(x^*) = J(x^*)^T J(x^*)$ ist positiv definit. Dann existiert $B(x^*)^{\frac 12}$. Es gilt:

\begin{align*}
 H(x^*) &= B(x^*) + E(x^*) \\
&= B^{\frac 12} (x^*) \l(I + B(x^*)^{-\frac 12} E(x^*)  B(x^*)^{-\frac 12} \r) B(x^*)^{\frac 12} \text{ positiv definit} \\
&\LRA I + B(x^*)^{-\frac 12} E(x^*) B(x^*)^{\frac 12} \text{ positiv definit} \\
 \end{align*}

Die Umformung

\[B(x^*)^{-1} E(x^*) = B(x^*)^{-\frac 12} \l(B(x^*)^{-\frac 12} E(x^*) B(x^*)^{-\frac 12} \r) B(x^*)^{\frac 12} \]

ist eine Ähnlichkeitstransformation, daher hat $B(x^*)^{-1} E(x^*)$ die selben Eigenwerte wie

\[ B(x^*)^{-\frac 12} E(x^*) B(x^*)^{-\frac 12} \]

Also ist

\[ \rho (B(x^*)^{-\frac 12} E(x^*) B(x^*)^{-\frac 12} ) < 1 \]

und die Eigenwerte von

\[ I + B(x^*)^{-\frac 12} E(x^*) B(x^*)^{-\frac 12} \]

liegen in $(0,2)$, also ist $H(x^*)$ positiv definit.

\msubsection{Korollar 5.14}

In Satz $5.14$ ist $x^*$ nicht nur stationärer Punkt sondern striktes lokales Minimum und stabil gegen Störungen.

Beweis: hinreichende Bedinung zweiter ordnung für lokale Minima für unbeschränkte Optimierungsprobleme, siehe unten.

\msubsection{Fazit}

Für Kleine-Residuen-Probleme

\bitm
\item konvergiert das Newton-Verfahren mit $M = (B+E)^{-1}$ gegen ein stabiles lokales Minimum 
\item konvergiert das Gauß-Newton-Verfahren
\eitm

Für Große-Residuen-Problem ($\kappa>1$ bzw. $\rho (B(x^*)^{-1} E(x^*)) > 1$) konvergiert das Gauß-Newton-Verfahren nicht. Das Newton-Verfahren konvergiert lokal gegen ein Minimum $x^*$. Was passiert in diesem Punkt?

\msection{Statistische Störung des Problems}

Sei $F(x) = ( \eta_i - h_i(x))_{i=1,\cdots,M}$, (o.\,B.\,d.\,A. $\sigma_i \equiv 1$), $\eta:$ Messdaten

wobei mit den wahren werten $\ov x: \eta_j - h_i(\ov x) \sim \mathcal N(0,1)$ und unabhängig

\[ F(x^*) = \eta - h(x^*), \quad \eta = h(x^*) + F(x^*) \]

Spiegel die Messfehler an den geschätzten Modellantworten.

\begin{align*}
\hat \eta :&= h(x^*) - F(x^*) \\
F(x^*) &= h(x^*) - \hat \eta
\end{align*}

$\hat \eta$: gestörte Messdaten.

Dann gilt:

\[ \| \hat \eta - h(x^*) \| = \|F(x^*)\| = \|\eta + h(x^*) \| \]

Betrachte eine Homotopie:

\[ \tilde F(x,\tau) := F(x) + (\tau-1) F(x^*), \quad \tau \in [-1,1] \]

zwischen Originalproblem und gespiegeltem Problem:

\begin{align*}
\tau &= +1: \quad \tilde F(x,+1) = F(x) = \eta - h(x) \\
\tau &= -1: \quad \tilde F(x,-1) = F(x) - 2 F(x^*) = \eta - h(x) - \eta + h(x^*) - h(x^*) + \hat \eta \\
&= \hat \eta - h(x)
\end{align*}

Betrachte die Probleme:

\[ \min_x \frac 12 \|\tilde F(x,\tau)\|_2^2, \quad \tau \in [-1,1] \quad (5.13) \]

\msection{Satz 5.15}

Sei $x^*$ ein Minimum von

\[ \min \frac 12 \| F(x)\|_2^2 \]

mit $\tilde \kappa := \rho( B(x^*)^{-1} E(x^*) ) > 1$. Dann gilt:

\bitm
\item a) Für alle $\tau$ ist $x^*$ ein stationärer Punkt von $\min \frac 12 \|\tilde F(x,\tau)\|_2^2$
\item b) Die Hessematrix in $x^*$ ist $\tilde H(x^*,\tau) = B(x^*) + \tau E(x^*)$
\item c) $\tilde H(x^*, \tau)$ ist für alle $\tau < - \frac 1\kappa \geq -1 $ nicht positiv definit 
\eitm

Beweis: Es gilt 

\begin{align*}
J(x^*)^T F(x^*) &= 0 \\
\tilde J(x,\tau) &= J(x) \\
\tilde J(x^*), \tau)^T \tilde F(x^*, \tau) &= J(x^*)^T ( F(x^*) + (\tau-1) F(x^*)) = 0 \RA \text{ a)} \\
\tilde H(x,\tau) &= \tilde J(x,\tau)^T \tilde J(x,\tau)  + \tilde F(x,\tau) \frac{\partial \tilde J}{\partial x} (x,\tau) \\
&= B(x) + (F(x) + (\tau-1) F(x^*)) \frac{\partial J}{\partial x}(x) \\
\tilde H(x^*,\tau) &= B(x^*) + \tau F(x^*) \frac{\partial J}{\partial x}(x^*) \\
&= B(x^*) + \tau E(x^*) \RA \text{ b)} \\
& \tilde H(x^*, \tau) \text{ positiv definit} \\
\LRA & \underbrace{I + B(x^*)^{-\frac 12} \tau E(x^*) B(x^*)^{-\frac 12}}_{=: I + \tau \hat E(x^*)} \text{ positiv definit} \\
\rho\l( B(x^*)^{-1} E(x^*)\r) &= \rho\l( B(x^*)^{-\frac 12} E(x^*) B(x^*)^{-\frac 12} \r) \\
&= \rho \l( \hat E(x^*) \r) \\
&= \tilde \kappa > 1 \\
\end{align*}

$I+\tau \hat E(x^*)$ hat einen Eigenwert $1+\tau \tilde \kappa$, dieser ist $<0$ falls $\tau < - \frac{1}{\tilde \kappa} \geq -1$, dann ist $\tilde H(x^*,\tau)$ nicht positiv definit $\RA$ c)

\msubsection{Fazit}

Für Große-Residuen-Probleme ist das Minimum von $\min \frac 12 \|F(x)\|_2^2$ nicht stabil gegen Störungen. $x^*$ kann ein Sattelpunkt oder ein Maximum werden. Das Minimum springt von $x^*$ weg. Große-Residuen-Minima sind nicht statistisch stabil. Sie sind Minima, aber keine Parameterschätzer. Das Gauß-Newton-Verfahren konvergiert nicht gegen sie.























































